######### Multi-Agent Debate RL Training Config #########
# Base .run config for GRPO training of debate and baseline experiments.
#
# PURPOSE: This file documents our experiment-specific training settings.
# The SWEEP configs in Plan 03 use the production reference .run file
# (8x15_math_data.run) as their run_config= base, and apply our experiment
# settings from this file via patch_run_config.
#
# This file serves as the source of truth for our training parameters â€”
# Plan 03's SWEEP configs read these values into their patch_run_config dicts.
#
# Fields marked [SWEEP-OVERRIDABLE] can be overridden by SWEEP configs.
# Fields not marked are base defaults applied to all experiments.

######### Fields you may want to edit after generation #########
wandb.run_name = "{user}_multiagent_debate_rl_{date}_{time}"
wandb.project_name = "multiagent-debate-rl"
output_dir = "s3://us-east-01a/30d/post-training/${USER}/multiagent-debate-rl/${SWEEP_NAME}/${TRIAL_IDX}"

# -- minizord -- #
minizord.eval.name = 'flink_eval'
minizord.eval.n_generation_steps = 4
use_fixed_eval_set = False

export_last_step = False

# [SWEEP-OVERRIDABLE] Batch sizes (default: 64 for our smaller dataset)
train_batch_size = 64
eval_batch_size = 64

performance.stack_layer_params_at_init = False

# -- sampling config during training -- #
sample.decode_loop_unroll = 4

# -- reward model --
# CRITICAL: reward is computed by Comb environment, not a separate model
reward_model.name = 'dummyrewardmodel'

# -- loss -- #
objective.loss.name = "placeholder"
# EXPERIMENT SETTINGS (from this file, applied to both debate and baseline):
# - beta: 0.01 (our locked KL coefficient, not 0.0 like reference)
# - generations_per_prompt: 8 (GRPO setting)
# - loss_variation: "gspo_use_ref_policy_as_pi_old" (required when beta > 0)
objective.loss.kwargs = {"preference": {"use_reference_policy": True,
                                        "beta": 0.01,
                                        "avg_loglikelihood": False,
                                        "generations_per_prompt": 8,
                                        "loss_variation": "gspo_use_ref_policy_as_pi_old"},
                         "n_grad_steps_per_batch": 1,
                         "kl_top_k": 0,
                         "ema_tau": 0.,
                        }

# -- L2 regularization -- #
l2_trainable_params_loss.reference_ckpt = ""
l2_trainable_params_loss.enabled = False
l2_trainable_params_loss.alpha = 0.0

# -- checkpoint saving -- #
# Keep last + best checkpoints (not all like reference's -1)
n_last_ckpts_to_keep = 2

# Advanced logging (same as reference - gives gradient norms, histograms, etc.)
advanced_logging.enabled = True
advanced_logging.norm_granularity = ("global",)
advanced_logging.norm_target_trees = ("grad", "update")
advanced_logging.log_opt_aux_metrics = True
advanced_logging.log_moe_histograms = True
advanced_logging.log_histograms = True
advanced_logging.hist_bins = 20
advanced_logging.hist_granularity = ("global",)
advanced_logging.hist_interval = 1
advanced_logging.hist_proportion = 0.01
advanced_logging.hist_target_trees = ("grad",)

########## Fields that are generated by command_data #########
# -- metadata for deployer and tracking -- #
# (Same as reference - using same tokenizer and model generation)
metadata.blobheart_tokenizer_id = "multilingual+255k+bos+eos+sptok+fim+agents3"
metadata.experimental_options = {'format_for_pretraining': False,
 'use_chat_packing': True,
 'use_special_tokens': True,
 'whitespace_mode': 'NONE'}
metadata.packing_algorithm = ""
metadata.command_dataset_version = ""
metadata.model_type = "OfflinePreferenceModelBuilder"
metadata.model_generation = "c3"
metadata.model_size = "26A91T"
metadata.team_name = "C3"
metadata.reasoning = True

# -- data -- #
# [SWEEP-OVERRIDABLE] Sequence length (8192 for math, not 32k like reference)
max_sequence_length = 8192
tokenizer_path = "gs://cohere-prod/encoders/releases/0.6.0/m255k_bos_eos_fim_agents3_special_tokens_with_eot_template-ns.json"

# CRITICAL: CombLineEncoder for parsing CommandDataCombItem JSONL
data_loader.name = "rl_dataloader"
data_loader.kwargs = {"debug": False, "json_encoder_name": "CombLineEncoder"}

# Data paths (placeholders - SWEEP configs will override with actual paths)
# SWEEP configs will set these to converted Comb JSONL files from Plan 02-01
data_dir_dict = {
    "${DATA_DIR}/train.jsonl": 1,
}
eval_data_dir_dict = {
    "${DATA_DIR}/eval.jsonl": 1,
}

# -- lr schedule -- #
lr_schedule.name = 'cosine_decay'
lr_schedule.kwargs = {'warmup_steps': 10, 'total_steps': 500, 'peak_lr': 1e-6, 'end_lr': 5e-7}

# [SWEEP-OVERRIDABLE] Training steps (500 default for MATH 500, SWEEP can override)
total_train_steps = 500
n_validation_steps = 10

# [SWEEP-OVERRIDABLE] Checkpoint and validation frequency
# NOTE: hard_update_ref_every_steps in loss kwargs should match export_every_steps
checkpoint_every_steps = 50
validation_every_steps = 50
log_loss_and_steptime_every_steps = 1

# -- output -- #
fax_bot.enabled = False

# -- wandb -- #
wandb.enabled = True

# -- profiling -- #
enable_ray_profiling = False
profile_every_steps = -1
first_step_to_profile = 1000000

# -- architecture -- #
# (Same as reference)
objective.last_layer = "projection_to_vocab"
use_compilation_cache = True
use_flash_attention = True

# -- finetuning -- #
finetuning.n_last_layers = None
finetuning.early_stopping.metric_name = None
finetuning.early_stopping.n_steps_to_track = None
finetuning.reset_step_number = True
force_dont_restore_opt_state = True

# -- megazord -- #
gather_params_along_dp_for_forward_pass = False
n_tensor_parallel = 1
write_ckpts_with_tensorstore = True
skip_compiling_functions = False

# -- batch -- #
n_gradient_accumulation_steps = 1

# -- hook frequency -- #
log_loss_and_steptime_aggregate_mode = "average"
log_grad_norms = False

# -- optimizer -- #
# (Same as reference - adam with gradient clipping)
optimizer.name = 'adam'
optimizer.kwargs = {'b1': 0.9, 'b2': 0.95, 'eps': 1e-08, 'additive_weight_decay': 0.1, 'gradient_clip_norm': 1.0}

# -- dropout -- #
residual_dropout_rate = 0.0
experimental_fast_rng = False

# -- RNG seed -- #
# Match our fixed seed convention from Phase 1
seed = 42

# -- Quantization -- #
# (Same as reference)
quantize.params = True
quantize.activations = True
quantize.alibi_activations = True

# -- MoE -- #
# (Same as reference)
moe.capacity_factor = 1.0
moe.experts_per_tok = 2
moe.load_loss_scale = 0
moe.router_bias_update_speed = 0
moe.scan_over_experts = False

# Auto-resume support
read_extra_state = False
