######### Qwen3-4B CooperBench â€” Docker-based verifiable rewards, solo mode #########
wandb.run_name = "{user}_flink_cooperbench_qwen3_4b_{date}_{time}"
wandb.project_name = "multiagent-debate-rl"
output_dir = "gs://co-train-oktodelete-15d/{user}/cooperbench/qwen3-4b/{date}_{time}"

# -- minizord -- #
minizord.eval.name = "flink_eval"
minizord.eval.n_generation_steps = 4
use_fixed_eval_set = False

train_batch_size = 4
eval_batch_size = 4

# -- sampling config during training -- #
sample.decode_loop_unroll = 4

# -- reward model --
reward_model.name = "dummyrewardmodel"

# -- loss -- #
objective.loss.name = "UNUSED"
objective.loss.kwargs = {
    "preference": {
        "use_reference_policy": True,
        "beta": 0.03,
        "avg_loglikelihood": False,
        "generations_per_prompt": 4,
    },
    "n_grad_steps_per_batch": 1,
    "kl_top_k": 0,
    "ema_tau": 0.0,
}

l2_trainable_params_loss.reference_ckpt = ""
l2_trainable_params_loss.enabled = False
l2_trainable_params_loss.alpha = 0.0

# -- checkpoint saving --
n_last_ckpts_to_keep = 2

########## Fields for Qwen3-4B #########
metadata.blobheart_tokenizer_id = "qwen3-4b"
metadata.experimental_options = {
    "format_for_pretraining": False,
    "use_chat_packing": True,
    "use_special_tokens": True,
    "whitespace_mode": "NONE",
}
metadata.packing_algorithm = ""
metadata.command_dataset_version = ""
metadata.model_type = "OfflinePreferenceModelBuilder"
metadata.model_size = "4B"
metadata.reasoning = True
metadata.eos_token = "<|im_end|>"
metadata.eot_token = "<|im_end|>"

# -- data -- #
max_sequence_length = 4096
tokenizer_path = (
    "gs://cohere-dev-central-2/users/terry/qwen3-4b-instruct-eot/tokenizer.json"
)
data_loader.name = "rl_dataloader"
data_loader.kwargs = {"debug": False, "json_encoder_name": "CombLineEncoder"}

data_dir_dict = {
    "gs://cohere-command/data/generated_jsonl_data/cooperbench_full_v1/train.jsonl": 1.0,
}
eval_data_dir_dict = {
    "gs://cohere-command/data/generated_jsonl_data/cooperbench_full_v1/train.jsonl": 1.0,
}

# -- lr schedule -- #
lr_schedule.name = "cosine_decay"
lr_schedule.kwargs = {"warmup_steps": 10, "total_steps": 100, "peak_lr": 1e-6, "end_lr": 1e-7}
total_train_steps = 100
n_validation_steps = 10
checkpoint_every_steps = 25
validation_every_steps = 25
log_loss_and_steptime_every_steps = 1

# -- output -- #
fax_bot.enabled = False
# -- wandb -- #
wandb.enabled = True
# -- profiling -- #
enable_ray_profiling = False
profile_every_steps = -1
first_step_to_profile = 100000
# -- architecture -- #
objective.last_layer = "projection_to_vocab"
use_compilation_cache = True
use_flash_attention = True

# -- finetuning -- #
finetuning.n_last_layers = None
finetuning.early_stopping.metric_name = None
finetuning.early_stopping.n_steps_to_track = None
finetuning.reset_step_number = True
force_dont_restore_opt_state = True

# -- megazord -- #
sharding.n_tensor_parallel = 1
write_ckpts_with_tensorstore = True
skip_compiling_functions = False

# -- batch -- #
n_gradient_accumulation_steps = 1

# -- hook frequency -- #
log_loss_and_steptime_aggregate_mode = "average"
log_grad_norms = False

# -- optimizer -- #
optimizer.name = "adam"
optimizer.kwargs = {"b1": 0.9, "b2": 0.95, "eps": 1e-08, "additive_weight_decay": 0.1, "gradient_clip_norm": 1.0}

# -- dropout -- #
residual_dropout_rate = 0.0
experimental_fast_rng = False

# -- RNG seed -- #
seed = 42

# -- Quantization (disabled for Qwen3 4B) -- #
quantize.params = False
quantize.activations = False
quantize.alibi_activations = False
