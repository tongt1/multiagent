---
phase: 03-analysis-modules
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/analyze_fig5.py
  - data/fig5_metrics.json
autonomous: true
requirements: [FIG5-01, FIG5-02, FIG5-03, FIG5-04]

must_haves:
  truths:
    - "Comm vs no-comm success rates are computed with Wilson 95% CIs"
    - "Merge conflict rates are computed for coop-comm (41%) and coop-nocomm (55%) with Wilson CIs"
    - "Agent messages are classified into speech act types using regex heuristics with priority ordering"
    - "Communication overhead is computed as messages_count / total_steps per task"
    - "Each message gets exactly one classification (no double-counting)"
    - "Records with eval_error are excluded from rate computations"
    - "Speech act percentages sum to 100% across plan/question/update/other categories"
  artifacts:
    - path: "scripts/analyze_fig5.py"
      provides: "Figure 5 analysis script computing success rates, merge conflict rates, speech acts, overhead"
      min_lines: 150
    - path: "data/fig5_metrics.json"
      provides: "Structured JSON output consumed by Phase 4 figure generation"
  key_links:
    - from: "scripts/analyze_fig5.py"
      to: "data/results.json"
      via: "json.load reads unified results store"
      pattern: "json\\.load.*results\\.json"
    - from: "scripts/analyze_fig5.py"
      to: "data/fig5_metrics.json"
      via: "json.dump writes metrics output"
      pattern: "json\\.dump.*fig5_metrics"
---

<objective>
Compute all Figure 5 statistical metrics: comm vs no-comm success rates, merge conflict rates with and without communication, speech act classification of agent messages, and communication overhead as percentage of action budget.

Purpose: Produces the data for Figure 5's three panels: (a) success rate comparison, (b) merge conflict reduction from communication, (c) communication overhead breakdown. The merge conflict rate is the strongest signal in our data (41% comm vs 55% no-comm).

Output: `scripts/analyze_fig5.py` (rerunnable analysis script) and `data/fig5_metrics.json` (structured metrics for Phase 4).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-analysis-modules/03-RESEARCH.md
@data/results.json
@scripts/collect_results.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Figure 5 analysis script with speech act classifier</name>
  <files>scripts/analyze_fig5.py</files>
  <action>
Create `scripts/analyze_fig5.py` -- a pure-Python script (stdlib only: json, math, re, collections, pathlib, argparse, sys) that reads `data/results.json` and writes `data/fig5_metrics.json`.

**1. Wilson CI function:**
Same implementation as analyze_fig4.py (copy the function -- no shared module needed for 15 lines):
```python
def wilson_ci(successes: int, total: int) -> tuple[float, float]:
```

**2. Speech act classification (regex heuristics):**

Define three compiled regex patterns:
```python
PLAN_PATTERNS = re.compile(
    r'\b('
    r'I will|I am going to|I\'ll|I plan to|I intend to|'
    r'I will be|I\'m going to|my plan|my approach|my strategy|'
    r'Let me|I am modifying|I am implementing|I am adding|'
    r'I have implemented|I\'ve implemented|'
    r'I suggest|I recommend|I propose|'
    r'my task is|I\'m working on|I am working on'
    r')\b',
    re.IGNORECASE,
)

QUESTION_PATTERNS = re.compile(
    r'\?|'
    r'\b('
    r'can you|could you|would you|will you|do you|did you|'
    r'is there|are there|have you|shall we|should we|'
    r'please let me know|please confirm|please ensure|'
    r'does this|does that|is this|is that'
    r')\b',
    re.IGNORECASE,
)

UPDATE_PATTERNS = re.compile(
    r'\b('
    r'I have completed|I have finished|I\'m done|I\'ve done|'
    r'I completed|done with|completed the|finished the|'
    r'I have made|I have added|changes are complete|'
    r'changes done|modifications complete|is ready|'
    r'has been implemented|my task is complete|'
    r'I\'ve added|I\'ve made|I\'ve modified|'
    r'I have resolved|acknowledged|proceeding with'
    r')\b',
    re.IGNORECASE,
)
```

Classification function with priority ordering (question > update > plan > other):
```python
def classify_speech_act(message_text: str) -> str:
    if QUESTION_PATTERNS.search(message_text):
        return "question"
    if UPDATE_PATTERNS.search(message_text):
        return "update"
    if PLAN_PATTERNS.search(message_text):
        return "plan"
    return "other"
```
Each message gets exactly ONE classification -- no double-counting.

**3. Success rate computation (FIG5-01):**
- Filter records: exclude `eval_error is not None` and `infra_error is True`
- Group by setting: `coop-comm` and `coop-nocomm`
- Count `both_passed == True` as successes
- Compute rate and Wilson CI for each setting
- Expected: both 0% with our data, but pipeline must handle it

**4. Merge conflict rate computation (FIG5-02):**
- Filter records: exclude `eval_error is not None` and `infra_error is True`
- Group by setting: `coop-comm` and `coop-nocomm`
- Count `merge_outcome == "merge_failed"` as conflicts
- Compute conflict rate = conflicts / total and Wilson CI
- Expected: coop-comm 41%, coop-nocomm 55% (this is the strongest signal)
- Also compute the difference and its direction

**5. Speech act classification (FIG5-03):**
- Collect ALL messages from coop-comm records (regardless of eval_error -- messages exist even for errored records)
- For each message: classify using `classify_speech_act(msg["message"])`
- Count per category: plan, question, update, other
- Compute percentages (must sum to 100%)
- Expected from research: ~62% plan, ~11% question, ~7% update, ~20% other

**6. Communication overhead (FIG5-04):**
- For each coop-comm record that has messages_count > 0 and total_steps > 0:
  - overhead_pct = messages_count / total_steps * 100
- Compute aggregate stats: mean, median, min, max, std
- Also store per-task overhead for distribution visualization in Phase 4
- Expected: mean ~22.8%, range 2.9% to 53.8%

**7. Output schema (data/fig5_metrics.json):**
```json
{
    "success_rates": {
        "coop_comm": {"successes": N, "total": N, "rate": F, "ci_lower": F, "ci_upper": F},
        "coop_nocomm": {"successes": N, "total": N, "rate": F, "ci_lower": F, "ci_upper": F}
    },
    "merge_conflict_rates": {
        "coop_comm": {"conflicts": N, "total": N, "rate": F, "ci_lower": F, "ci_upper": F},
        "coop_nocomm": {"conflicts": N, "total": N, "rate": F, "ci_lower": F, "ci_upper": F},
        "difference": {"value": F, "direction": "comm reduces conflicts by X%"}
    },
    "speech_acts": {
        "total_messages": N,
        "plan": {"count": N, "pct": F},
        "question": {"count": N, "pct": F},
        "update": {"count": N, "pct": F},
        "other": {"count": N, "pct": F}
    },
    "overhead": {
        "mean_pct": F,
        "median_pct": F,
        "min_pct": F,
        "max_pct": F,
        "std_pct": F,
        "n_tasks": N,
        "per_task": [{"task_id": N, "repo": S, "messages": N, "total_steps": N, "overhead_pct": F}, ...]
    },
    "metadata": {
        "input_file": "data/results.json",
        "total_records": N,
        "coop_comm_records": N,
        "coop_nocomm_records": N,
        "speech_act_method": "regex heuristics with priority ordering (question > update > plan > other)"
    }
}
```

**8. CLI and console output:**
- Support `--input` and `--output` arguments with defaults
- Print human-readable summary: success rates, conflict rates with difference, speech act distribution, overhead stats
  </action>
  <verify>
  - `python scripts/analyze_fig5.py` runs without errors and prints summary
  - `python -c "import json; d=json.load(open('data/fig5_metrics.json')); print(d['merge_conflict_rates']['coop_comm']['rate'])"` returns approximately 0.41
  - `python -c "import json; d=json.load(open('data/fig5_metrics.json')); print(d['merge_conflict_rates']['coop_nocomm']['rate'])"` returns approximately 0.55
  - `python -c "import json; d=json.load(open('data/fig5_metrics.json')); sa=d['speech_acts']; print(sa['plan']['count'] + sa['question']['count'] + sa['update']['count'] + sa['other']['count'] == sa['total_messages'])"` returns True (no double-counting)
  - `python -c "import json; d=json.load(open('data/fig5_metrics.json')); print(abs(d['speech_acts']['plan']['pct'] + d['speech_acts']['question']['pct'] + d['speech_acts']['update']['pct'] + d['speech_acts']['other']['pct'] - 100.0) < 0.1)"` returns True (sums to ~100%)
  - `python -c "import json; d=json.load(open('data/fig5_metrics.json')); print(d['overhead']['mean_pct'])"` returns value near 22.8
  </verify>
  <done>scripts/analyze_fig5.py produces data/fig5_metrics.json with comm vs no-comm success rates, merge conflict rates (41% vs 55%), speech act classification (plan/question/update/other), and communication overhead (~22.8% mean). All rates include Wilson 95% CIs. Speech act categories sum to 100%.</done>
</task>

<task type="auto">
  <name>Task 2: Validate Figure 5 metrics against known data properties</name>
  <files></files>
  <action>
Run inline validation to confirm all Figure 5 metrics are consistent with known data.

**Validation checks:**

1. **Success rates:** Both coop-comm and coop-nocomm success rates should be 0.0 (0 passed out of ~100 each). Totals should match non-error record counts.
2. **Merge conflict rates:** coop-comm conflicts should be 41 (41%), coop-nocomm should be 55 (55%). Cross-check against raw data:
   ```python
   import json
   with open('data/results.json') as f:
       records = json.load(f)
   comm_conflicts = sum(1 for r in records if r['setting'] == 'coop-comm' and r['merge_outcome'] == 'merge_failed' and r.get('eval_error') is None and not r.get('infra_error'))
   ```
   This must match the fig5_metrics value.
3. **Conflict difference:** coop-nocomm rate - coop-comm rate should be positive (~0.14), confirming communication reduces conflicts.
4. **Speech act totals:** Total messages classified should equal 428 (total from data). If different, investigate -- some records may have eval_error but still have messages. Both are valid, just document which approach.
5. **Speech act categories:** plan% + question% + update% + other% = 100% (within rounding tolerance 0.1%)
6. **No double-counting:** Sum of category counts == total_messages
7. **Overhead bounds:** mean_pct should be between 10% and 50%. No individual task overhead should exceed 100% (messages can't exceed total_steps).
8. **Wilson CI validity:** For all rates, ci_lower >= 0, ci_upper <= 1, ci_lower <= rate <= ci_upper.
9. **Overhead per-task count:** Number of per-task overhead entries should match number of coop-comm records with messages_count > 0.

Print PASS/FAIL for each check. If any FAIL, investigate and fix.
  </action>
  <verify>
  - All 9 validation checks print PASS
  - No assertion errors
  </verify>
  <done>Figure 5 metrics validated: correct merge conflict rates, speech acts sum to 100%, overhead within expected bounds, Wilson CIs valid. Ready for Phase 4 figure generation.</done>
</task>

</tasks>

<verification>
1. `scripts/analyze_fig5.py` exists and is executable
2. `data/fig5_metrics.json` exists with valid JSON
3. Merge conflict rates: coop-comm ~41%, coop-nocomm ~55%
4. Success rates: both 0% with correct denominators
5. Speech act categories sum to 100%, no double-counting
6. Overhead mean near 22.8% with per-task distribution
7. All Wilson CIs have valid bounds
8. Script is rerunnable (idempotent)
</verification>

<success_criteria>
- data/fig5_metrics.json contains success rates, merge conflict rates, speech acts, and overhead
- Merge conflict rates match raw data (strongest signal in the dataset)
- Speech act classification uses priority ordering, no double-counting
- Communication overhead computed both aggregate and per-task
- All rates include Wilson 95% CIs
- Script runs in < 5 seconds (pure computation, no API calls)
</success_criteria>

<output>
After completion, create `.planning/phases/03-analysis-modules/03-02-SUMMARY.md`
</output>
