---
phase: 03-analysis-modules
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/analyze_fig6.py
  - data/fig6_metrics.json
autonomous: true
requirements: [FIG6-01, FIG6-02]
user_setup:
  - service: cohere
    why: "LLM-based communication error classifier needs Cohere API access"
    env_vars:
      - name: CO_API_KEY
        source: "Cohere dashboard or existing environment (same key used in Phase 1 benchmark runs)"

must_haves:
  truths:
    - "An LLM classifier labels communication errors using the paper's C1a/C1b/C2/C3b/C4a/C4b taxonomy"
    - "The classifier is a NEW implementation, NOT reusing cooperbench-eval classifiers (different taxonomy)"
    - "All 100 coop-with-comm transcripts are classified"
    - "Per-category frequency counts are available for bar chart generation"
    - "Transcripts with no messages are handled gracefully (no errors, reported separately)"
    - "API failures are retried with exponential backoff and do not crash the script"
    - "Each transcript classification includes the evidence (quoted exchange) for each error found"
  artifacts:
    - path: "scripts/analyze_fig6.py"
      provides: "LLM-based communication error classifier and runner script"
      min_lines: 180
    - path: "data/fig6_metrics.json"
      provides: "Per-transcript classifications and aggregate frequency counts"
  key_links:
    - from: "scripts/analyze_fig6.py"
      to: "data/results.json"
      via: "json.load reads unified results store for coop-comm transcripts"
      pattern: "json\\.load.*results\\.json"
    - from: "scripts/analyze_fig6.py"
      to: "https://stg.api.cohere.com/v2/chat"
      via: "httpx POST for LLM classification"
      pattern: "httpx\\.Client.*post"
    - from: "scripts/analyze_fig6.py"
      to: "data/fig6_metrics.json"
      via: "json.dump writes classification results"
      pattern: "json\\.dump.*fig6_metrics"
---

<objective>
Implement an LLM-based communication error classifier using the paper's C1a/C1b/C2/C3b/C4a/C4b taxonomy, then run it on all 100 coop-with-comm transcripts to produce per-category frequency counts for Figure 6.

Purpose: Produces the data for Figure 6 (communication error taxonomy breakdown). This is a NEW classifier -- the existing cooperbench-eval classifiers use the same C-codes but for a DIFFERENT taxonomy (coordination failure modes from Table 1). The requirements specify a message-quality taxonomy: unanswered questions, vague responses, incorrect claims, and spammy repetition.

Output: `scripts/analyze_fig6.py` (classifier + runner script) and `data/fig6_metrics.json` (per-transcript classifications and frequency counts).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-analysis-modules/03-RESEARCH.md
@data/results.json
@scripts/collect_results.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement LLM communication error classifier and run on all transcripts</name>
  <files>scripts/analyze_fig6.py, data/fig6_metrics.json</files>
  <action>
Create `scripts/analyze_fig6.py` -- a Python script (using json, os, time, re, pathlib, argparse, sys from stdlib, plus httpx from venv) that classifies communication errors in all coop-comm transcripts.

**CRITICAL: This is a NEW classifier, NOT reusing cooperbench-eval classifiers.** The cooperbench-eval C-codes (C1a, C2, etc.) map to coordination failure modes (work_overlap, divergent_architecture). This script uses the REQUIREMENTS' taxonomy which describes MESSAGE QUALITY errors:
- C1a: Unanswered - No Reply (a direct question receives no reply at all)
- C1b: Unanswered - Ignored (a request is acknowledged but substantively ignored)
- C2: Non-answer/Vague (response is vague, non-committal, lacks actionable info)
- C3b: Incorrect Claim - Corrected (factually wrong claim about codebase/changes)
- C4a: Spammy - Same Info (repeats information already communicated)
- C4b: Spammy - Near-duplicate (near-identical content, copy-paste blocks)

**1. Configuration:**
```python
COHERE_API_URL = os.environ.get("COHERE_API_URL", "https://stg.api.cohere.com/v2/chat")
TAXONOMY_MODEL = os.environ.get("TAXONOMY_MODEL", "command-a-03-2025")
MAX_RETRIES = 3
RETRY_DELAY = 2.0
BASE_RATE_LIMIT = 0.5  # seconds between requests
```

**2. Taxonomy prompt builder:**
Build the classification prompt for each transcript. Include:
- The 6-category taxonomy with descriptions
- The formatted transcript: `[agent1 -> agent2] message text`
- Request JSON response format: `{"errors": [{"category": "C1a|...", "evidence": "...", "message_index": N}], "summary": "..."}`
- Instruction: if no errors found, return `{"errors": [], "summary": "No errors detected"}`

**3. classify_transcript function:**
- Takes list of message dicts (with 'from', 'to', 'message' keys) and api_key
- Builds taxonomy prompt
- Sends to Cohere API via httpx with:
  - model: TAXONOMY_MODEL
  - system message: "You are an expert at analyzing multi-agent communication transcripts. Respond only in valid JSON."
  - temperature: 0.0
  - max_tokens: 1024
- Parses JSON response with fallback: direct parse -> code block extraction -> find first { to last }
- Returns dict with 'errors' list and 'summary'
- On failure after retries: returns `{"errors": [], "summary": "Classification failed: {error}", "error": true}`

**4. JSON response parser:**
```python
def _parse_json_response(text: str) -> dict:
```
- Try direct json.loads
- Try extracting from ```json ... ``` code block
- Try finding first { to last }
- Fallback: return error dict

**5. Main runner:**
- Load data/results.json
- Filter to coop-comm records that have messages_count > 0
- Get API key from CO_API_KEY env var (exit with clear error message if missing)
- Support `--dry-run` flag: print first transcript's prompt without making API calls
- Support `--limit N` flag: process only first N transcripts (for testing)
- For each transcript:
  - Call classify_transcript with rate limiting (BASE_RATE_LIMIT between calls)
  - Exponential backoff on failure (RETRY_DELAY * attempt)
  - Print progress: `[N/100] task_id REPO_NAME: X errors found`
  - Store result with task_id, repo, errors list
- Validate each classification: ensure all category codes are valid (C1a/C1b/C2/C3b/C4a/C4b)
- Invalid categories get logged as warnings but included in output

**6. Frequency aggregation:**
After classifying all transcripts:
- Count errors per category across all transcripts
- Compute percentage: category_count / total_errors * 100 (for proportion within errors)
- Also compute prevalence: transcripts_with_category / total_transcripts * 100 (for how common each error type is)
- Count transcripts with zero errors
- Count total errors found

**7. Output schema (data/fig6_metrics.json):**
```json
{
    "classifications": [
        {
            "task_id": 1655,
            "repo": "dottxt_ai_outlines_task",
            "messages_count": 4,
            "errors": [
                {"category": "C4a", "evidence": "...", "message_index": 2}
            ],
            "summary": "...",
            "api_error": false
        }
    ],
    "frequency": {
        "C1a": {"count": N, "pct_of_errors": F, "pct_of_transcripts": F},
        "C1b": {"count": N, "pct_of_errors": F, "pct_of_transcripts": F},
        "C2": {"count": N, "pct_of_errors": F, "pct_of_transcripts": F},
        "C3b": {"count": N, "pct_of_errors": F, "pct_of_transcripts": F},
        "C4a": {"count": N, "pct_of_errors": F, "pct_of_transcripts": F},
        "C4b": {"count": N, "pct_of_errors": F, "pct_of_transcripts": F}
    },
    "summary": {
        "total_transcripts": N,
        "transcripts_with_errors": N,
        "transcripts_without_errors": N,
        "total_errors": N,
        "api_failures": N,
        "mean_errors_per_transcript": F
    },
    "metadata": {
        "input_file": "data/results.json",
        "model": "command-a-03-2025",
        "api_url": "https://stg.api.cohere.com/v2/chat",
        "taxonomy": "C1a/C1b/C2/C3b/C4a/C4b (message quality errors, NOT coordination failures)",
        "classifier_note": "NEW classifier per requirements, not reusing cooperbench-eval classifiers"
    }
}
```

**8. Console output:**
- Print progress during classification
- Print final summary: frequency table, error counts, API failures
- Print total API cost estimate (rough: 100 calls at ~$0.01-0.02 each)

**9. CLI interface:**
```python
parser.add_argument("--input", type=Path, default=DEFAULT_INPUT)
parser.add_argument("--output", type=Path, default=DEFAULT_OUTPUT)
parser.add_argument("--dry-run", action="store_true", help="Print first prompt without API calls")
parser.add_argument("--limit", type=int, default=0, help="Process only first N transcripts (0=all)")
parser.add_argument("--resume", action="store_true", help="Resume from existing output, skip already classified")
```

**10. Resume support:**
If `--resume` is passed and output file exists:
- Load existing classifications
- Build set of already-classified task_ids
- Skip those during processing
- Merge new results with existing
This prevents re-doing expensive API calls if the script is interrupted.

**Execution:**
First, test with `--dry-run` to verify prompt formatting.
Then, test with `--limit 3` to verify API connectivity and response parsing.
Finally, run full: `python scripts/analyze_fig6.py` (all 100 transcripts).

The script MUST be run from the cooperbench venv (for httpx):
```bash
source /mnt/data/terry/home/cooperbench-repro/repos/CooperBench/.venv/bin/activate
python scripts/analyze_fig6.py
```
  </action>
  <verify>
  - `python scripts/analyze_fig6.py --dry-run` prints the taxonomy prompt for the first transcript without making API calls
  - `python scripts/analyze_fig6.py --limit 1` successfully classifies 1 transcript (tests API connectivity)
  - `python scripts/analyze_fig6.py` completes all 100 transcripts (may take 5-10 minutes with rate limiting)
  - `python -c "import json; d=json.load(open('data/fig6_metrics.json')); print(d['summary']['total_transcripts'])"` returns ~100 (coop-comm records with messages)
  - `python -c "import json; d=json.load(open('data/fig6_metrics.json')); print(sorted(d['frequency'].keys()))"` returns ['C1a', 'C1b', 'C2', 'C3b', 'C4a', 'C4b']
  - `python -c "import json; d=json.load(open('data/fig6_metrics.json')); print(d['summary']['api_failures'])"` returns 0 (or very low)
  </verify>
  <done>scripts/analyze_fig6.py classifies all coop-comm transcripts using the C1a/C1b/C2/C3b/C4a/C4b message-quality taxonomy via Cohere Command A. data/fig6_metrics.json contains per-transcript classifications with evidence, aggregate frequency counts, and summary statistics. Script supports --dry-run, --limit, and --resume for safe incremental execution.</done>
</task>

<task type="auto">
  <name>Task 2: Validate Figure 6 classifications and frequency counts</name>
  <files></files>
  <action>
Run inline validation to confirm Figure 6 classifications are well-formed and complete.

**Validation checks:**

1. **Transcript coverage:** Number of classified transcripts should match number of coop-comm records with messages_count > 0 in results.json. Cross-check:
   ```python
   import json
   with open('data/results.json') as f:
       records = json.load(f)
   expected = sum(1 for r in records if r['setting'] == 'coop-comm' and r.get('messages_count', 0) > 0)
   ```
2. **Valid categories only:** Every error in every classification has category in {C1a, C1b, C2, C3b, C4a, C4b}. No unexpected categories.
3. **All 6 categories present in frequency table:** Even if count is 0 for some categories.
4. **Frequency consistency:** Sum of all category counts in frequency table should equal summary.total_errors.
5. **API failures:** api_failures should be 0 (or document which transcripts failed and why).
6. **Evidence quality:** Spot-check 3 random classifications -- each error should have non-empty evidence string.
7. **No cooperbench-eval confusion:** No results contain "work_overlap", "divergent_architecture", or other cooperbench-eval classifier terms. If found, the wrong classifier was used.
8. **Summary consistency:** transcripts_with_errors + transcripts_without_errors == total_transcripts.

Print PASS/FAIL for each check. If category validation fails, investigate which transcripts produced invalid categories and whether the LLM output needs better parsing.
  </action>
  <verify>
  - All 8 validation checks print PASS
  - No unexpected classification categories
  - No cooperbench-eval taxonomy leakage
  </verify>
  <done>Figure 6 classifications validated: all transcripts classified, only valid C1a-C4b categories, frequency counts consistent, no cooperbench-eval taxonomy confusion. Ready for Phase 4 figure generation.</done>
</task>

</tasks>

<verification>
1. `scripts/analyze_fig6.py` exists with --dry-run, --limit, --resume support
2. `data/fig6_metrics.json` exists with valid JSON
3. All ~100 coop-comm transcripts classified (with messages)
4. Only valid taxonomy categories (C1a/C1b/C2/C3b/C4a/C4b)
5. Frequency counts are consistent with per-transcript details
6. API failures documented (ideally 0)
7. No cooperbench-eval taxonomy confusion
8. Script is re-entrant (--resume flag)
</verification>

<success_criteria>
- data/fig6_metrics.json contains per-transcript error classifications with evidence
- All 6 taxonomy categories are represented in frequency table
- Total transcripts classified matches coop-comm records with messages
- API failure rate < 5% (ideally 0%)
- Script supports --dry-run for prompt preview and --resume for interrupted runs
- Taxonomy is explicitly the message-quality variant (C1a=unanswered, etc.), NOT the cooperbench-eval coordination-failure variant
</success_criteria>

<output>
After completion, create `.planning/phases/03-analysis-modules/03-03-SUMMARY.md`
</output>
