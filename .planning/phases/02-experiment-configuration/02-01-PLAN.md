---
phase: 02-experiment-configuration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - configs/reward_shaping_sweep/__init__.py
  - configs/reward_shaping_sweep/_base.py
  - configs/reward_shaping_sweep/sweep_identity.py
  - configs/reward_shaping_sweep/sweep_difference_rewards.py
  - configs/reward_shaping_sweep/sweep_potential_based.py
  - configs/reward_shaping_sweep/sweep_coma_advantage.py
  - configs/reward_shaping_sweep/sweep_reward_mixing.py
  - tests/test_reward_shaping_sweep_configs.py
autonomous: true

must_haves:
  truths:
    - "Each of the 5 SWEEP configs can be imported as a Python module without errors"
    - "Diffing any two configs shows differences ONLY in reward_shaping_strategy and reward_shaping_params -- all other hyperparameters, data paths, queue settings, and model configs are identical"
    - "All 5 configs use sweep.Queue.post_training_cohere_labs_queue with priority_class='dev-low'"
    - "All 5 configs use SmolLM-135M settings (1 training GPU, 1 sampling GPU, 2048 seq len, GCS checkpoint)"
    - "potential_based config uses gamma=0.99 and potential_type='debate_length'"
    - "coma_advantage config uses n_rollouts_per_prompt=4 in both DebateMetricStreamerConfig field AND reward_shaping_params"
    - "reward_mixing config uses alpha=0.5"
    - "All 5 configs include env_name_remap={'math': 'math_debate'} for debate environment"
  artifacts:
    - path: "configs/reward_shaping_sweep/_base.py"
      provides: "Shared constants (GPU, batch, LR, queue, data path) imported by all 5 configs"
      contains: "SMOLLM_135M"
    - path: "configs/reward_shaping_sweep/sweep_identity.py"
      provides: "Identity baseline SWEEP config"
      contains: "class RewardShapingSweep"
    - path: "configs/reward_shaping_sweep/sweep_difference_rewards.py"
      provides: "Difference rewards SWEEP config"
      contains: "difference_rewards"
    - path: "configs/reward_shaping_sweep/sweep_potential_based.py"
      provides: "Potential-based SWEEP config"
      contains: "potential_based"
    - path: "configs/reward_shaping_sweep/sweep_coma_advantage.py"
      provides: "COMA advantage SWEEP config"
      contains: "coma_advantage"
    - path: "configs/reward_shaping_sweep/sweep_reward_mixing.py"
      provides: "Reward mixing SWEEP config"
      contains: "reward_mixing"
    - path: "tests/test_reward_shaping_sweep_configs.py"
      provides: "Validation tests for ECFG-01 through ECFG-08"
      contains: "test_configs_differ_only_in_reward_shaping"
  key_links:
    - from: "configs/reward_shaping_sweep/sweep_*.py"
      to: "configs/reward_shaping_sweep/_base.py"
      via: "import of shared constants"
      pattern: "from configs.reward_shaping_sweep._base import"
    - from: "configs/reward_shaping_sweep/_base.py"
      to: "configs/model_profiles.py"
      via: "SMOLLM_135M model profile"
      pattern: "from configs.model_profiles import SMOLLM_135M"
    - from: "configs/reward_shaping_sweep/sweep_*.py"
      to: "post_training.flink.components.debate_enrichment"
      via: "DebateMetricStreamerConfig with strategy-specific params"
      pattern: "DebateMetricStreamerConfig"
---

<objective>
Create a shared base constants module and 5 strategy-specific SWEEP config files (identity, difference_rewards, potential_based, coma_advantage, reward_mixing) for SmolLM-135M on MATH-500, plus a validation test suite confirming all 5 configs are identical except for reward shaping fields.

Purpose: Satisfy requirements ECFG-01 through ECFG-08 by producing 5 submission-ready SWEEP configs that enable an apples-to-apples comparison of reward shaping strategies on the flex queue.

Output: 7 config files in `configs/reward_shaping_sweep/` + 1 test file in `tests/`
</objective>

<execution_context>
@/home/terry_tong_cohere_com/.claude/get-shit-done/workflows/execute-plan.md
@/home/terry_tong_cohere_com/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-experiment-configuration/02-RESEARCH.md

# Reference existing SWEEP config for structure/imports
@configs/sweep_math_debate_grpo.py

# SmolLM-135M model profile for GPU/seq/ckpt values
@configs/model_profiles.py

# SmolLM run config for data paths and training hyperparams
@run_configs/smollm_135m_rloo_math.run

# DebateMetricStreamerConfig field definitions
# See: src/training/wandb_enrichment/debate_streamer.py (DebateMetricStreamerConfig class)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create shared base constants module and 5 strategy-specific SWEEP configs</name>
  <files>
    configs/reward_shaping_sweep/__init__.py
    configs/reward_shaping_sweep/_base.py
    configs/reward_shaping_sweep/sweep_identity.py
    configs/reward_shaping_sweep/sweep_difference_rewards.py
    configs/reward_shaping_sweep/sweep_potential_based.py
    configs/reward_shaping_sweep/sweep_coma_advantage.py
    configs/reward_shaping_sweep/sweep_reward_mixing.py
  </files>
  <action>
    Create `configs/reward_shaping_sweep/` directory with:

    **`__init__.py`** -- empty init file.

    **`_base.py`** -- shared constants module. Import SMOLLM_135M from `configs.model_profiles` and expose:
    - `_PROFILE = SMOLLM_135M`
    - `NUM_TRAINING_GPUS = _PROFILE.num_training_gpus` (1)
    - `NUM_SAMPLING_GPUS = _PROFILE.num_sampling_gpus` (1)
    - `MAX_SEQUENCE_LENGTH = _PROFILE.max_sequence_length` (2048)
    - `CKPT_PATH = _PROFILE.ckpt_path` (GCS SmolLM path)
    - `TRAIN_BATCH_SIZE = 2`
    - `EVAL_BATCH_SIZE = 2`
    - `TOTAL_TRAIN_STEPS = 15`
    - `LEARNING_RATE = 3e-6`
    - `KL_BETA = 0.03`
    - `GENERATIONS_PER_PROMPT = 4`
    - `EXPORT_EVERY_STEPS = 5`
    - `HARD_UPDATE_REF_EVERY_STEPS = 5`
    - `SEED = 42`
    - `N_GRADIENT_ACCUMULATION_STEPS = 1`
    - `PRIORITY_CLASS = "dev-low"`
    - `RUN_CONFIG_PATH = "${HOME}/repos/post_training/post_training/experimental/comb_flink/configs/smollm_135m_rloo_math.run"`
    - `MATH_500_DATA_PATH = "gs://cohere-dev-central-2/comb/data/math_500/2025_05_15/scenarios_train.jsonl"`
    - `K8S_SECRETS_PATH = "${HOME}/repos/secrets_template.toml"`
    - `WANDB_PROJECT = "multiagent-debate-rl"`

    Also include vLLM sidecar constants:
    - `_VLLM_SIDECAR = "vllm"`
    - `_VLLM_PORT = 8000`
    - `_VLLM_EXPORT_DIR = "/data/1d/post-training/${USER}/${SWEEP_NAME}/${TRIAL_IDX}"`

    **5 strategy config files** -- Each follows the EXACT same structure as `configs/sweep_math_debate_grpo.py` but adapted for SmolLM-135M. All imports come from `_base.py`. The ONLY differences between the 5 files are:

    1. **`sweep_identity.py`**: `DebateMetricStreamerConfig(n_rollouts_per_prompt=GENERATIONS_PER_PROMPT)` -- no reward_shaping_strategy or reward_shaping_params (defaults to identity passthrough).

    2. **`sweep_difference_rewards.py`**: `DebateMetricStreamerConfig(n_rollouts_per_prompt=GENERATIONS_PER_PROMPT, reward_shaping_strategy="difference_rewards", reward_shaping_params={})`

    3. **`sweep_potential_based.py`**: `DebateMetricStreamerConfig(n_rollouts_per_prompt=GENERATIONS_PER_PROMPT, reward_shaping_strategy="potential_based", reward_shaping_params={"gamma": 0.99, "potential_type": "debate_length"})`

    4. **`sweep_coma_advantage.py`**: `DebateMetricStreamerConfig(n_rollouts_per_prompt=GENERATIONS_PER_PROMPT, reward_shaping_strategy="coma_advantage", reward_shaping_params={"n_rollouts_per_prompt": GENERATIONS_PER_PROMPT})` -- NOTE: n_rollouts_per_prompt appears in BOTH the DebateMetricStreamerConfig field AND the reward_shaping_params dict. Both must equal GENERATIONS_PER_PROMPT (4).

    5. **`sweep_reward_mixing.py`**: `DebateMetricStreamerConfig(n_rollouts_per_prompt=GENERATIONS_PER_PROMPT, reward_shaping_strategy="reward_mixing", reward_shaping_params={"alpha": 0.5})`

    Each config file MUST:
    - Use `class RewardShapingSweep(sweep_base.Sweep)` as the class name
    - Set `sweep_output_path="${HOME}/sweep_jobs/reward_shaping_comparison/"`
    - Set `cluster=sweep.Cluster.cw_us_east_04_prod`
    - Set `partition=f"gpu_{NUM_TRAINING_GPUS}"` (resolves to "gpu_1")
    - Set `queue=sweep.Queue.post_training_cohere_labs_queue`
    - Set `priority_class=PRIORITY_CLASS` (resolves to "dev-low")
    - Set `run_config=RUN_CONFIG_PATH`
    - Set `ckpt_path=CKPT_PATH`
    - Include `env_name_remap={"math": "math_debate"}` in CombItemsPreprocessorConfig
    - Include `patch_data_item` with react_config (same as existing debate config)
    - Set sidecar partition to `f"gpu_{NUM_SAMPLING_GPUS}"` (resolves to "gpu_1")
    - Set `n_gradient_accumulation_steps=N_GRADIENT_ACCUMULATION_STEPS` (1, not 4 like the 7B config)
    - Include `if __name__ == "__main__": sweep.cli.run_sweep_with_flags(...)` at the bottom
    - Include identical `patch_run_config` dict with SmolLM-appropriate values:
      - `train_batch_size=TRAIN_BATCH_SIZE` (2)
      - `eval_batch_size=EVAL_BATCH_SIZE` (2)
      - `total_train_steps=TOTAL_TRAIN_STEPS` (15)
      - `max_sequence_length=MAX_SEQUENCE_LENGTH` (2048)
      - `validation_every_steps=15` (match total_train_steps for SmolLM)
      - `n_gradient_accumulation_steps=N_GRADIENT_ACCUMULATION_STEPS` (1)
      - `lr_schedule` with `peak_lr=LEARNING_RATE`, `end_lr=LEARNING_RATE`
      - `seed=SEED`
      - GRPO loss config with `beta=KL_BETA`, `generations_per_prompt=GENERATIONS_PER_PROMPT`
      - `hard_update_ref_every_steps=HARD_UPDATE_REF_EVERY_STEPS` (5)
      - `export_every_steps=EXPORT_EVERY_STEPS` (5)
      - `n_last_ckpts_to_keep=2`
      - `checkpoint_every_steps=15`
      - All other fields matching the existing debate config structure

    Do NOT include:
    - `advanced_logging` block (SmolLM does not need it for initial validation)
    - `MODEL_TO_QUERY` or `IS_STAGING` constants (not needed for SmolLM)
    - `hf_export.override_mesh_for_local_gathering` (SmolLM uses tp=1, no mesh override)

    CRITICAL: Every line of code in the 5 config files MUST be identical EXCEPT the DebateMetricStreamerConfig constructor arguments. To achieve this, consider defining a helper function in `_base.py` that builds the entire `RewardShapingSweep` class, or keep the class definition in each file but ensure all non-DebateMetricStreamerConfig code is copy-exact from a common template.
  </action>
  <verify>
    Run: `python -c "from configs.reward_shaping_sweep import sweep_identity, sweep_difference_rewards, sweep_potential_based, sweep_coma_advantage, sweep_reward_mixing; print('All 5 configs import successfully')"` from the repo root.
    Also verify: `python -c "from configs.reward_shaping_sweep._base import NUM_TRAINING_GPUS, PRIORITY_CLASS; assert NUM_TRAINING_GPUS == 1; assert PRIORITY_CLASS == 'dev-low'; print('Base constants correct')"`
  </verify>
  <done>
    7 files exist in `configs/reward_shaping_sweep/`. All 5 strategy configs can be imported. Base constants expose SmolLM-135M values with dev-low priority. Each strategy config uses the correct DebateMetricStreamerConfig arguments per ECFG-01 through ECFG-05.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create validation test suite for config consistency</name>
  <files>
    tests/test_reward_shaping_sweep_configs.py
  </files>
  <action>
    Create `tests/test_reward_shaping_sweep_configs.py` with tests covering:

    **Test 1: `test_all_five_configs_importable`** (ECFG-01 through ECFG-05)
    - Import all 5 config modules
    - Instantiate `RewardShapingSweep` from each
    - Assert no exceptions

    **Test 2: `test_configs_differ_only_in_reward_shaping`** (ECFG-06)
    - For each pair of configs, compare their `fax` attribute (the PostTraining config)
    - Use `model_dump()` or dict serialization to get the full config as a dict
    - Walk the dict tree and collect all keys that differ between any two configs
    - Assert that ALL differing keys contain "reward_shaping" in their path
    - This is the critical test for the apples-to-apples comparison requirement

    **Test 3: `test_all_configs_use_dev_low_priority`** (ECFG-07)
    - For each config, assert `config.fax.priority_class == "dev-low"`
    - For each config, assert `config.fax.queue == sweep.Queue.post_training_cohere_labs_queue`

    **Test 4: `test_all_configs_use_smollm_135m`** (ECFG-07/08)
    - Assert all configs use `partition="gpu_1"` (not gpu_8 or gpu_16)
    - Assert CKPT_PATH contains "smollm-135M"
    - Assert MAX_SEQUENCE_LENGTH == 2048

    **Test 5: `test_strategy_specific_params`** (ECFG-03, ECFG-04, ECFG-05)
    - Extract DebateMetricStreamerConfig from each config's actor_outputs_streamers
    - Assert identity: no reward_shaping_strategy or empty string
    - Assert difference_rewards: reward_shaping_strategy == "difference_rewards"
    - Assert potential_based: reward_shaping_strategy == "potential_based", params == {"gamma": 0.99, "potential_type": "debate_length"}
    - Assert coma_advantage: reward_shaping_strategy == "coma_advantage", params == {"n_rollouts_per_prompt": 4}
    - Assert reward_mixing: reward_shaping_strategy == "reward_mixing", params == {"alpha": 0.5}

    **Test 6: `test_all_configs_use_math_debate_env`** (ECFG-08)
    - Extract CombItemsPreprocessorConfig from each config
    - Assert env_name_remap == {"math": "math_debate"}

    **Test 7: `test_n_rollouts_per_prompt_consistent`** (Pitfall 3/4 from research)
    - For each config, assert DebateMetricStreamerConfig.n_rollouts_per_prompt == GENERATIONS_PER_PROMPT (4)
    - For coma_advantage, additionally assert reward_shaping_params["n_rollouts_per_prompt"] == GENERATIONS_PER_PROMPT

    NOTE: The tests need to handle the fact that SWEEP config classes may not be easily serializable without the sweep infrastructure. If direct instantiation fails due to missing sweep dependencies, fall back to:
    - Importing the module and inspecting its class attributes at the Python level
    - Using `inspect.getsource()` or AST-level checks
    - Parsing the config files as text and comparing non-DebateMetricStreamerConfig sections

    Prefer structural tests (import + attribute access) over text-based tests. But use whatever approach works given the available dependencies.
  </action>
  <verify>
    Run: `python -m pytest tests/test_reward_shaping_sweep_configs.py -v` from the repo root. All tests must pass.
  </verify>
  <done>
    Validation test file exists with 7+ tests covering ECFG-01 through ECFG-08. All tests pass. The critical `test_configs_differ_only_in_reward_shaping` test confirms that diffing any two configs shows differences ONLY in reward shaping fields.
  </done>
</task>

</tasks>

<verification>
1. All 5 config files exist in `configs/reward_shaping_sweep/`
2. `python -c "from configs.reward_shaping_sweep import sweep_identity"` succeeds (repeat for all 5)
3. `python -m pytest tests/test_reward_shaping_sweep_configs.py -v` passes all tests
4. Manual spot-check: `diff configs/reward_shaping_sweep/sweep_identity.py configs/reward_shaping_sweep/sweep_coma_advantage.py` shows differences ONLY in DebateMetricStreamerConfig args
</verification>

<success_criteria>
- 5 SWEEP configs exist, one per strategy (ECFG-01 through ECFG-05)
- Configs are identical except for reward_shaping_strategy and reward_shaping_params (ECFG-06)
- All use post_training_cohere_labs_queue with dev-low priority (ECFG-07)
- All reference MATH-500 data from GCS via SmolLM-135M run config (ECFG-08)
- Validation tests confirm all of the above programmatically
</success_criteria>

<output>
After completion, create `.planning/phases/02-experiment-configuration/02-01-SUMMARY.md`
</output>
