---
phase: 03-observability-and-comparison
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/training/wandb_enrichment/debate_streamer.py
  - src/training/wandb_enrichment/workspace_template.py
  - src/training/wandb_enrichment/metric_schema.py
  - tests/test_debate_streamer.py
autonomous: true

must_haves:
  truths:
    - "DebateMetricStreamer calls wandb.config.update() with reward_shaping_strategy on first get()"
    - "Workspace template includes shaped reward comparison panels (mean + per-role)"
    - "Shaped reward metric strings (e.g. debate/shaped_reward/mean) are centralized as importable constants, not hardcoded in multiple places"
  artifacts:
    - path: "src/training/wandb_enrichment/debate_streamer.py"
      provides: "wandb.config.update() call with strategy name and params"
      contains: "wandb.config.update"
    - path: "src/training/wandb_enrichment/workspace_template.py"
      provides: "Shaped reward LinePlot panels in workspace dashboard"
      contains: "Shaped Reward"
    - path: "src/training/wandb_enrichment/metric_schema.py"
      provides: "METRIC_SHAPED_REWARD_* constants"
      contains: "METRIC_SHAPED_REWARD_MEAN"
    - path: "tests/test_debate_streamer.py"
      provides: "Tests for wandb.config.update() and shaped reward panel presence"
      contains: "test_wandb_config_update"
  key_links:
    - from: "src/training/wandb_enrichment/debate_streamer.py"
      to: "wandb.config"
      via: "wandb.config.update() in first get() call, guarded by wandb.run check"
      pattern: "wandb\\.config\\.update"
    - from: "src/training/wandb_enrichment/workspace_template.py"
      to: "src/training/wandb_enrichment/metric_schema.py"
      via: "imports METRIC_SHAPED_REWARD_* constants for panel y-axis"
      pattern: "METRIC_SHAPED_REWARD"
---

<objective>
Add strategy metadata to WandB run config and extend workspace template with shaped reward comparison panels.

Purpose: Without this, runs from different reward shaping strategies are indistinguishable in the WandB UI. The strategy name is buried in nested minizord config, and no dashboard panels exist for shaped rewards. This plan addresses OBSV-01 (shaped reward distinct from raw), OBSV-02 (strategy name in run metadata), and partially OBSV-03 (dashboard panels).

Output: Modified debate_streamer.py with wandb.config.update(), extended workspace_template.py with 2 shaped reward panels, new metric constants in metric_schema.py, and tests.
</objective>

<execution_context>
@/home/terry_tong_cohere_com/.claude/get-shit-done/workflows/execute-plan.md
@/home/terry_tong_cohere_com/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-observability-and-comparison/03-RESEARCH.md
@src/training/wandb_enrichment/debate_streamer.py
@src/training/wandb_enrichment/workspace_template.py
@src/training/wandb_enrichment/metric_schema.py
@tests/test_debate_streamer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add shaped reward metric constants and wandb.config.update() to streamer</name>
  <files>
    src/training/wandb_enrichment/metric_schema.py
    src/training/wandb_enrichment/debate_streamer.py
  </files>
  <action>
    **Step 1: Add shaped reward metric constants to metric_schema.py.**

    Add a new section "Shaped Reward Metrics" after the "Rollout Strategy Metrics" section in metric_schema.py. Define these constants:
    - `METRIC_SHAPED_REWARD_MEAN = f"{DEBATE_PREFIX}shaped_reward/mean"` -- Mean shaped reward across batch
    - `METRIC_SHAPED_REWARD_SOLVER = f"{DEBATE_PREFIX}shaped_reward/solver"` -- Per-role shaped reward for solver
    - `METRIC_SHAPED_REWARD_VERIFIER = f"{DEBATE_PREFIX}shaped_reward/verifier"` -- Per-role shaped reward for verifier
    - `METRIC_SHAPED_REWARD_JUDGE = f"{DEBATE_PREFIX}shaped_reward/judge"` -- Per-role shaped reward for judge
    - `METRIC_SHAPED_REWARD_STRATEGY_ACTIVE = f"{DEBATE_PREFIX}shaped_reward/strategy_active"` -- 0.0 for identity, 1.0 for others

    Also add all 5 constants to the `ALL_DEBATE_METRICS` list at the bottom under a "# Shaped rewards" comment.

    **Step 2: Add _update_wandb_config() method to _DebateMetricStreamerImpl in debate_streamer.py.**

    Add a new private method `_update_wandb_config(self)` to the `_DebateMetricStreamerImpl` class. This method:
    1. Tries to import wandb (lazy import inside method)
    2. Checks `if wandb.run is not None`
    3. Calls `wandb.config.update({"reward_shaping_strategy": self._reward_shaper.name, "reward_shaping_params": getattr(self._config, "reward_shaping_params", {}) or {}}, allow_val_change=True)`
    4. Logs info: `f"DebateMetricStreamer: updated WandB config with reward_shaping_strategy='{self._reward_shaper.name}'"`
    5. Wraps entire body in try/except Exception, logging warning on failure

    **Step 3: Call _update_wandb_config() in get() on first call.**

    In the `get()` method, insert `self._update_wandb_config()` inside the existing `if self._get_count == 1:` block (line 135 of debate_streamer.py), immediately AFTER the `init_debate_workspace()` call (line 138) and still within the same try/except. The insertion point is after line 140 (`logger.warning(...)` for workspace init failure) — add the `_update_wandb_config()` call right after the workspace init try/except but still inside the `if self._get_count == 1:` guard. This ensures:
    - wandb.init() has already been called by Flink (it runs before first get())
    - The workspace template is created first
    - The config update happens exactly once

    **Critical: Do NOT call wandb.config.update() in __init__(). The wandb.run does not exist during __init__ -- it is only available after Flink calls wandb.init() which happens between __init__ and the first get().**

    **Step 4: REFACTOR existing hardcoded metric strings in _compute_and_apply_shaped_rewards().**

    The method `_compute_and_apply_shaped_rewards()` (debate_streamer.py lines 268-343) already logs shaped reward metrics using hardcoded `"debate/shaped_reward/*"` strings. This step REFACTORS those existing strings to use the new constants — it does NOT create shaped reward logging from scratch. The existing logging logic and control flow must remain unchanged.

    Replace hardcoded strings with constants:
    - Line 307: `f"debate/shaped_reward/{role}"` -> use a lookup dict `{"solver": METRIC_SHAPED_REWARD_SOLVER, "verifier": METRIC_SHAPED_REWARD_VERIFIER, "judge": METRIC_SHAPED_REWARD_JUDGE}` and index by `role`
    - Line 311: `"debate/shaped_reward/mean"` -> `METRIC_SHAPED_REWARD_MEAN`
    - Line 326: `"debate/shaped_reward/mean"` -> `METRIC_SHAPED_REWARD_MEAN`
    - Line 332: `f"debate/shaped_reward/{role}"` -> same lookup dict approach as line 307
    - Line 341: `"debate/shaped_reward/strategy_active"` -> `METRIC_SHAPED_REWARD_STRATEGY_ACTIVE`

    Add imports at the top of debate_streamer.py:
    ```python
    from src.training.wandb_enrichment.metric_schema import (
        METRIC_SHAPED_REWARD_MEAN,
        METRIC_SHAPED_REWARD_SOLVER,
        METRIC_SHAPED_REWARD_VERIFIER,
        METRIC_SHAPED_REWARD_JUDGE,
        METRIC_SHAPED_REWARD_STRATEGY_ACTIVE,
    )
    ```
  </action>
  <verify>
    Run `cd /home/terry_tong_cohere_com/reward-training && uv run python -c "from src.training.wandb_enrichment.metric_schema import METRIC_SHAPED_REWARD_MEAN, METRIC_SHAPED_REWARD_STRATEGY_ACTIVE; print(METRIC_SHAPED_REWARD_MEAN, METRIC_SHAPED_REWARD_STRATEGY_ACTIVE)"` -- should print `debate/shaped_reward/mean debate/shaped_reward/strategy_active`.

    Run `cd /home/terry_tong_cohere_com/reward-training && uv run python -c "from src.training.wandb_enrichment.debate_streamer import _DebateMetricStreamerImpl; assert hasattr(_DebateMetricStreamerImpl, '_update_wandb_config')"` -- should pass without error.

    Run existing tests: `cd /home/terry_tong_cohere_com/reward-training && uv run pytest tests/test_debate_streamer.py -x -q` -- all existing tests must still pass.
  </verify>
  <done>
    metric_schema.py has 5 METRIC_SHAPED_REWARD_* constants and they appear in ALL_DEBATE_METRICS. debate_streamer.py has _update_wandb_config() method called on first get(), and _compute_and_apply_shaped_rewards() uses imported constants instead of hardcoded strings. All existing tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend workspace template with shaped reward panels and add tests</name>
  <files>
    src/training/wandb_enrichment/workspace_template.py
    tests/test_debate_streamer.py
  </files>
  <action>
    **Step 1: Add shaped reward imports to workspace_template.py.**

    Add to the existing import block from metric_schema:
    ```python
    from src.training.wandb_enrichment.metric_schema import (
        # ... existing imports ...
        METRIC_SHAPED_REWARD_MEAN,
        METRIC_SHAPED_REWARD_SOLVER,
        METRIC_SHAPED_REWARD_VERIFIER,
        METRIC_SHAPED_REWARD_JUDGE,
    )
    ```

    **Step 2: Add Section 3.5 "Reward Shaping" to create_debate_workspace().**

    Insert a new section AFTER Section 3 (Training Health) and BEFORE Section 4 (Rollout Samples). Add:

    ```python
    # =====================================================================
    # Section 4: Reward Shaping Comparison
    # =====================================================================
    workspace.blocks = workspace.blocks + [
        wr.H1(text="Reward Shaping"),
        wr.LinePlot(
            title="Mean Shaped Reward by Strategy",
            x="Step",
            y=[METRIC_SHAPED_REWARD_MEAN],
            smoothing_factor=0.6,
            range_x=None,
            range_y=None,
            legend_position="north",
        ),
        wr.LinePlot(
            title="Per-Role Shaped Rewards",
            x="Step",
            y=[
                METRIC_SHAPED_REWARD_SOLVER,
                METRIC_SHAPED_REWARD_VERIFIER,
                METRIC_SHAPED_REWARD_JUDGE,
            ],
            smoothing_factor=0.6,
            range_x=None,
            range_y=None,
            legend_position="north",
        ),
    ]
    ```

    Renumber Section 4 comment to Section 5. Update the docstring at the top of create_debate_workspace() to mention "5 sections" instead of "4 sections" and add "4. Reward Shaping: Shaped reward comparison across strategies" to the section list.

    **Step 3: Add tests to tests/test_debate_streamer.py.**

    Add 3 new test functions at the end of the file:

    **test_wandb_config_update_called_on_first_get:**
    - Create a DebateMetricStreamer with `reward_shaping_strategy="difference_rewards"`
    - Mock `wandb.run` to be truthy (use `unittest.mock.patch("wandb.run", new=MagicMock())` and `unittest.mock.patch("wandb.config")`)
    - Call `streamer.get()` once
    - Assert `wandb.config.update` was called with a dict containing `"reward_shaping_strategy": "difference_rewards"`
    - Call `streamer.get()` a second time
    - Assert `wandb.config.update` was called exactly once (not on subsequent get() calls)

    **test_wandb_config_update_skipped_when_no_run:**
    - Create a DebateMetricStreamer with `reward_shaping_strategy="potential_based"`
    - Mock `wandb.run` to be `None`
    - Call `streamer.get()`
    - Assert `wandb.config.update` was NOT called

    **test_workspace_template_has_shaped_reward_panels:**
    - Read `src/training/wandb_enrichment/workspace_template.py` source as text
    - Assert the source contains `"METRIC_SHAPED_REWARD_MEAN"` (import used)
    - Assert the source contains `"Mean Shaped Reward by Strategy"` (panel title)
    - Assert the source contains `"Per-Role Shaped Rewards"` (panel title)
    - Assert the source contains `"METRIC_SHAPED_REWARD_SOLVER"` (per-role panel)

    **Important for the wandb mock tests:** The DebateMetricStreamer's first get() also calls `init_debate_workspace()` which tries to import wandb. Mock `src.training.wandb_enrichment.workspace_init.init_debate_workspace` as a no-op in both wandb-related tests to isolate the config update behavior.
  </action>
  <verify>
    Run `cd /home/terry_tong_cohere_com/reward-training && uv run pytest tests/test_debate_streamer.py -x -q` -- all tests must pass including the 3 new ones.

    Run `cd /home/terry_tong_cohere_com/reward-training && uv run pytest tests/test_debate_streamer.py -k "wandb_config" -v` -- should show 2 passing tests for wandb config update behavior.

    Run `cd /home/terry_tong_cohere_com/reward-training && uv run pytest tests/test_debate_streamer.py -k "workspace_template" -v` -- should show 1 passing test for workspace template content.
  </verify>
  <done>
    workspace_template.py has 2 new LinePlot panels (mean shaped reward + per-role shaped rewards) using metric_schema constants. 3 new tests validate: (1) wandb.config.update called once with strategy name on first get(), (2) skipped when wandb.run is None, (3) workspace template source contains shaped reward panels. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_debate_streamer.py -x -q` -- all tests pass
2. `uv run pytest tests/test_reward_shaping_sweep_configs.py -x -q` -- existing sweep config tests still pass (no regressions)
3. `uv run python -c "from src.training.wandb_enrichment.metric_schema import ALL_DEBATE_METRICS; assert 'debate/shaped_reward/mean' in ALL_DEBATE_METRICS"` -- shaped reward metrics registered
4. `grep -c 'wandb.config.update' src/training/wandb_enrichment/debate_streamer.py` -- returns 1 (exactly one config update call)
5. `grep -c 'Shaped Reward' src/training/wandb_enrichment/workspace_template.py` -- returns at least 1 (panel section exists)
</verification>

<success_criteria>
- OBSV-01: Shaped reward metrics use METRIC_SHAPED_REWARD_* constants from metric_schema.py, distinct from raw METRIC_REWARD_* constants
- OBSV-02: wandb.config.update() surfaces reward_shaping_strategy as a top-level WandB run config key on first get()
- OBSV-03 (partial): Workspace template has 2 shaped reward comparison panels for side-by-side viewing
- No regressions: All existing debate_streamer and sweep config tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-observability-and-comparison/03-01-SUMMARY.md`
</output>
