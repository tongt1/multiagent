---
phase: 03-observability-and-comparison
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - scripts/submit_reward_shaping_sweep.py
  - tests/test_reward_shaping_sweep_configs.py
autonomous: false

must_haves:
  truths:
    - "All 5 SWEEP configs can be submitted to the cluster via a single script"
    - "All 5 WandB runs are in the same project and have reward_shaping_strategy in run config"
    - "Training loss, accuracy, and shaped reward curves are visible side-by-side"
  artifacts:
    - path: "scripts/submit_reward_shaping_sweep.py"
      provides: "Batch submission and post-run validation for all 5 reward shaping SWEEP configs"
      contains: "submit_all"
    - path: "tests/test_reward_shaping_sweep_configs.py"
      provides: "Test that all 5 configs share WANDB_PROJECT constant"
      contains: "test_all_configs_share_wandb_project"
  key_links:
    - from: "scripts/submit_reward_shaping_sweep.py"
      to: "configs/reward_shaping_sweep/sweep_*.py"
      via: "subprocess call to uv run python <config> --submit start"
      pattern: "sweep_.*\\.py"
    - from: "scripts/submit_reward_shaping_sweep.py"
      to: "wandb.Api"
      via: "Post-run validation querying WandB for runs with reward_shaping_strategy config"
      pattern: "wandb\\.Api"
---

<objective>
Create batch submission script for all 5 SWEEP configs and validate the WandB comparison dashboard after runs complete.

Purpose: Without a batch submission script, each config must be submitted manually (5 separate commands). Without validation, there is no automated check that all 5 runs completed and have the expected metrics. This plan addresses the operational side of OBSV-03 (runs actually submitted and comparable).

Output: scripts/submit_reward_shaping_sweep.py (batch submission + dry-run + validation), additional WANDB_PROJECT consistency test, and human-verified WandB dashboard comparison.
</objective>

<execution_context>
@/home/terry_tong_cohere_com/.claude/get-shit-done/workflows/execute-plan.md
@/home/terry_tong_cohere_com/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-observability-and-comparison/03-RESEARCH.md
@.planning/phases/03-observability-and-comparison/03-01-SUMMARY.md
@configs/reward_shaping_sweep/_base.py
@scripts/
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create batch submission script and WANDB_PROJECT validation test</name>
  <files>
    scripts/submit_reward_shaping_sweep.py
    tests/test_reward_shaping_sweep_configs.py
  </files>
  <action>
    **Step 1: Create scripts/submit_reward_shaping_sweep.py.**

    Create a new script with the following structure:

    ```python
    #!/usr/bin/env python3
    """Batch submission and validation for reward shaping SWEEP comparison experiment.

    Usage:
        # Dry run (print commands without submitting):
        uv run python scripts/submit_reward_shaping_sweep.py --dry-run

        # Submit all 5 configs:
        uv run python scripts/submit_reward_shaping_sweep.py --submit

        # Validate runs after completion:
        uv run python scripts/submit_reward_shaping_sweep.py --validate
    """
    ```

    Define `SWEEP_CONFIGS` list with all 5 config paths:
    - `configs/reward_shaping_sweep/sweep_identity.py`
    - `configs/reward_shaping_sweep/sweep_difference_rewards.py`
    - `configs/reward_shaping_sweep/sweep_potential_based.py`
    - `configs/reward_shaping_sweep/sweep_coma_advantage.py`
    - `configs/reward_shaping_sweep/sweep_reward_mixing.py`

    Define `EXPECTED_STRATEGIES` set: `{"identity", "difference_rewards", "potential_based", "coma_advantage", "reward_mixing"}`

    Import `WANDB_PROJECT` from `configs.reward_shaping_sweep._base` to get the canonical project name.

    Implement 3 functions:

    **submit_all(dry_run: bool = False) -> None:**
    - For each config in SWEEP_CONFIGS:
      - Build command: `["uv", "run", "python", config_path, "--submit", "start"]`
      - If dry_run: print command and continue
      - Else: run via `subprocess.run(cmd, check=True)`, print success/failure
    - Print summary: "All 5 configs submitted" or "Dry run complete"

    **validate_runs() -> None:**
    - Import wandb, create `wandb.Api()`
    - Query runs: `api.runs(f"cohere/{WANDB_PROJECT}", filters={"config.reward_shaping_strategy": {"$exists": True}})`
    - Collect strategies found: `{run.config.get("reward_shaping_strategy") for run in runs}`
    - Check each expected strategy is present
    - Print per-strategy status: run name, status, step count
    - Print summary: "All 5 strategies found" or "Missing: {missing}"

    **main():**
    - argparse with mutually exclusive group: `--dry-run`, `--submit`, `--validate`
    - Default (no args): print usage and exit
    - Route to submit_all(dry_run=True), submit_all(dry_run=False), or validate_runs()

    Make the script executable with `if __name__ == "__main__": main()`.

    **Step 2: Add WANDB_PROJECT consistency test to tests/test_reward_shaping_sweep_configs.py.**

    Add a new test `test_all_configs_share_wandb_project` that:
    1. Reads each of the 5 SWEEP config source files as text
    2. Asserts each contains `from configs.reward_shaping_sweep._base import` (uses shared base)
    3. Asserts each contains `WANDB_PROJECT` in its source (references the shared constant)
    4. Reads `_base.py` and extracts the WANDB_PROJECT value via regex or AST
    5. Asserts the value equals `"multiagent-debate-rl"`

    This is a source-level test (consistent with the existing AST-based validation approach since `sweep` is not available locally).
  </action>
  <verify>
    Run `cd /home/terry_tong_cohere_com/reward-training && uv run python scripts/submit_reward_shaping_sweep.py --dry-run` -- should print 5 submit commands without actually running them.

    Run `cd /home/terry_tong_cohere_com/reward-training && uv run pytest tests/test_reward_shaping_sweep_configs.py -k "wandb_project" -v` -- new test passes.

    Run `cd /home/terry_tong_cohere_com/reward-training && uv run pytest tests/test_reward_shaping_sweep_configs.py -x -q` -- all existing + new tests pass.
  </verify>
  <done>
    scripts/submit_reward_shaping_sweep.py exists with --dry-run, --submit, and --validate modes. Dry run prints all 5 commands. test_all_configs_share_wandb_project validates all configs use the same WANDB_PROJECT constant from _base.py. All tests pass.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Submit all 5 runs and verify WandB comparison dashboard</name>
  <files>n/a (no code changes -- submission and verification only)</files>
  <action>
    This is a human verification checkpoint. All code changes for Phase 3 are complete:
    1. debate_streamer.py calls wandb.config.update() with strategy name on first get()
    2. workspace_template.py has shaped reward comparison panels
    3. metric_schema.py has METRIC_SHAPED_REWARD_* constants
    4. submit_reward_shaping_sweep.py can batch-submit all 5 configs

    The user should submit all 5 runs via the submission script, wait for completion, and verify the WandB dashboard.
  </action>
  <verify>
    1. Submit all 5 runs:
       `uv run python scripts/submit_reward_shaping_sweep.py --submit`

    2. Wait for all 5 runs to complete (SmolLM-135M with 15 steps, minutes not hours). Monitor via SWEEP UI or WandB.

    3. Validate runs programmatically:
       `uv run python scripts/submit_reward_shaping_sweep.py --validate`
       Expected: "All 5 strategies found" with each strategy showing completed status.

    4. Open WandB project: https://wandb.ai/cohere/multiagent-debate-rl

    5. Verify OBSV-02: In the Runs table, check that `reward_shaping_strategy` appears as a column. You should be able to group/filter runs by this column.

    6. Verify OBSV-01: Click into any non-identity run. Confirm `debate/shaped_reward/mean` metric exists and its values differ from `debate/reward/solver` (the raw reward). For the identity run, shaped_reward should equal raw reward.

    7. Verify OBSV-03: Check the "Debate Training Dashboard" report. Confirm the "Reward Shaping" section has two panels:
       - "Mean Shaped Reward by Strategy" showing curves for all 5 runs
       - "Per-Role Shaped Rewards" showing solver/verifier/judge shaped rewards

    8. Verify all 5 runs show distinct learning curves (identity baseline should differ from the other 4 strategies).

    Resume signal: Type "approved" if all 5 runs completed and the WandB dashboard shows comparable curves, or describe issues.
  </verify>
  <done>
    All 5 SmolLM-135M runs completed on flex queue. WandB dashboard shows: reward_shaping_strategy as filterable config column (OBSV-02), shaped_reward metrics distinct from raw rewards (OBSV-01), and all 5 runs visible in same project with comparable curves (OBSV-03). User confirmed via checkpoint.
  </done>
</task>

</tasks>

<verification>
1. `uv run python scripts/submit_reward_shaping_sweep.py --dry-run` -- prints 5 submit commands
2. `uv run pytest tests/ -x -q` -- all tests pass (debate_streamer + sweep_configs)
3. After submission: `uv run python scripts/submit_reward_shaping_sweep.py --validate` -- all 5 strategies found
4. WandB UI: reward_shaping_strategy visible as run config column
5. WandB UI: shaped reward panels present in workspace template
6. WandB UI: 5 runs with distinct learning curves in same project
</verification>

<success_criteria>
- OBSV-01: Each run has debate/shaped_reward/* metrics distinct from debate/reward/* (verified via --validate or WandB UI)
- OBSV-02: reward_shaping_strategy appears as filterable/groupable config column in WandB runs table
- OBSV-03: All 5 runs visible in same WandB project with comparable training loss, accuracy, and shaped reward curves
- All 5 SmolLM-135M runs completed successfully on flex queue
</success_criteria>

<output>
After completion, create `.planning/phases/03-observability-and-comparison/03-02-SUMMARY.md`
</output>
