---
phase: 01-execution-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - scripts/run_cooperbench.sh
autonomous: true

must_haves:
  truths:
    - "The orchestrator script runs cooperbench with correct arguments for all 3 settings"
    - "Retry logic re-invokes cooperbench up to 3 times per setting, skipping completed tasks each time"
    - "A smoke test of 5 pairs completes end-to-end with Docker backend producing result.json files"
  artifacts:
    - path: "scripts/run_cooperbench.sh"
      provides: "Orchestrator with retry logic, cost reporting, and infra_error tagging"
  key_links:
    - from: "scripts/run_cooperbench.sh"
      to: "repos/CooperBench/.venv/bin/cooperbench"
      via: "subprocess invocation"
      pattern: "cooperbench run"
    - from: "scripts/run_cooperbench.sh"
      to: "repos/CooperBench/logs/"
      via: "result.json files from benchmark runs"
      pattern: "logs/command-a-"
---

<objective>
Create the orchestrator script that runs CooperBench across all 3 experimental settings with retry logic, then validate the pipeline with a smoke test of 5 pairs in solo mode.

Purpose: The orchestrator encapsulates retry policy (3 attempts per setting), cost reporting, and infra_error tagging. The smoke test validates the entire pipeline (CLI -> Docker sandbox -> agent -> result) before committing to full runs.
Output: A working orchestrator script and verified end-to-end pipeline with 5 pairs completing in solo mode.
</objective>

<execution_context>
@/home/terry_tong_cohere_com/.claude/get-shit-done/workflows/execute-plan.md
@/home/terry_tong_cohere_com/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-execution-infrastructure/01-RESEARCH.md
@.planning/phases/01-execution-infrastructure/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create orchestrator script with retry logic, cost reporting, and infra_error tagging</name>
  <files>scripts/run_cooperbench.sh</files>
  <action>
Create `scripts/run_cooperbench.sh` — the main orchestrator for running all benchmark settings. This is a bash script, NOT a Python wrapper. It calls the upstream `cooperbench run` CLI directly.

**Script structure:**

1. **Configuration section** at the top:
   - `COOPERBENCH_DIR` pointing to `repos/CooperBench/`
   - `VENV_ACTIVATE` pointing to `.venv/bin/activate`
   - Agent: `mini_swe_agent` (per user decision — openhands_sdk is Modal-only)
   - Model: `command-a-03-2025`
   - Subset: `lite`
   - Backend: `docker`
   - Concurrency: `4`
   - Max attempts: `3` (for retry logic)
   - `--no-auto-eval` flag (eval is a separate step)
   - Environment variables: `MSWEA_MODEL_API_BASE`, `MSWEA_COST_TRACKING`, `LITELLM_LOG`

2. **Argument parsing** for selecting which setting(s) to run:
   - `--setting solo|coop-comm|coop-nocomm|all` (default: `all`)
   - `--smoke N` to limit to first N pairs for smoke testing (creates a temporary subset file with N pairs)
   - `--dry-run` to print commands without executing
   - If `--smoke` is provided, create a temporary subset JSON by reading the first N entries from `dataset/subsets/lite.json` and writing to a temp file, then pass `-s` pointing to that temp file. Alternatively, just let the full run go and rely on the user to Ctrl+C after N pairs complete, but the cleaner approach is to create a temp subset.

3. **Run function** `run_setting(name, setting_args)`:
   - Takes run name and setting-specific args
   - Loops up to `MAX_ATTEMPTS` times
   - Each iteration: `cd $COOPERBENCH_DIR && source $VENV_ACTIVATE && cooperbench run -n $NAME -m $MODEL -a $AGENT -s $SUBSET --backend docker -c $CONCURRENCY $SETTING_ARGS --no-auto-eval`
   - After each attempt: count completed tasks (result.json files with non-Error status) and remaining tasks
   - If all tasks completed (no Error status remaining), break early
   - Log attempt number, timestamp, completed/remaining counts

4. **Settings execution order** (per user decision):
   - Solo first: `run_setting "command-a-solo" "--setting solo"`
   - Coop with comm: `run_setting "command-a-coop-comm" "--setting coop"`
   - Coop no comm: `run_setting "command-a-coop-nocomm" "--setting coop --no-messaging"`

5. **Post-processing: infra_error tagging:**
   After all runs complete, scan all `result.json` files. For each file where `status == "Error"`:
   - Check if the error message matches infrastructure patterns:
     - Docker patterns: `OOM`, `killed`, `container`, `docker`, `timeout`
     - API patterns: `429`, `rate limit`, `connection`, `timeout`, `502`, `503`
   - If matches: add `"infra_error": true` field to the result.json
   - If no match (genuine failure): leave as-is
   - Print summary: N pass, N fail, N infra_error per setting

6. **Cost summary:**
   After all runs, scan all `result.json` files and sum up cost fields. Print per-setting and total cost. The cost field is at `result["agent"]["cost"]` or `result["cost"]` (check both patterns).

**Important decisions per user context:**
- Agent is `mini_swe_agent` (NOT `openhands_sdk` — per critical update, openhands_sdk is Modal-only)
- No budget ceiling — track costs but don't halt execution
- Retry = re-invoke `cooperbench run` (upstream resume skips completed tasks, retries Error tasks)
- Task timeout: 10 minutes — check if cooperbench CLI supports a timeout flag; if not, note this as a limitation

**Working directory:** The script must `cd repos/CooperBench/` before running cooperbench commands because dataset paths are relative.
  </action>
  <verify>
Run `bash scripts/run_cooperbench.sh --dry-run --setting solo` and confirm it prints the correct cooperbench command with all expected arguments (mini_swe_agent, command-a-03-2025, lite, docker, concurrency 4, solo setting, --no-auto-eval). Verify the retry loop structure by reading the script.
  </verify>
  <done>
`scripts/run_cooperbench.sh` exists with: (1) correct cooperbench CLI invocation for all 3 settings using `mini_swe_agent` and Docker backend, (2) retry loop of up to 3 attempts per setting with resume support, (3) post-processing for infra_error tagging, (4) cost summary reporting, (5) `--smoke` flag for limited pair testing, (6) `--dry-run` flag for command preview.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run smoke test of 5 pairs in solo mode to validate the full pipeline</name>
  <files></files>
  <action>
Run the orchestrator in smoke test mode to validate the complete pipeline end-to-end:

```bash
bash scripts/run_cooperbench.sh --smoke 5 --setting solo
```

This should:
1. Activate the cooperbench venv
2. Create a temporary 5-pair subset from lite.json
3. Run `cooperbench run` in solo mode with Docker backend on those 5 pairs
4. Produce result.json files in `repos/CooperBench/logs/command-a-solo/solo/`

**What to verify after the smoke test:**
- At least 5 result.json files exist under `repos/CooperBench/logs/command-a-solo/`
- Each result.json contains a valid JSON object with fields like `status`, `agent`, `cost`
- No systematic failures (e.g., all tasks failing with the same error suggests a config problem)
- Docker containers were created and cleaned up (no orphaned cooperbench containers)

**If the smoke test fails:**
- Check COHERE_API_KEY is set
- Check MSWEA_MODEL_API_BASE is correct for staging
- Check Docker daemon is running and images are available
- Check working directory is repos/CooperBench/
- Read error messages from cooperbench output and fix accordingly

**Important:** The smoke test validates that Command A can actually be called via litellm through the staging endpoint, that Docker sandboxes work, and that result files are written correctly. This is the critical integration point.

After the smoke test completes, inspect 1-2 result.json files to understand the output format for downstream phases.
  </action>
  <verify>
Check that result.json files exist: `find repos/CooperBench/logs/command-a-solo/ -name result.json | head -10`. Read one result.json to confirm it has valid structure with status, agent, and cost fields.
  </verify>
  <done>
Smoke test of 5 pairs completed end-to-end in solo mode. Result.json files exist with valid structure. The pipeline (cooperbench CLI -> Docker sandbox -> mini_swe_agent -> Command A -> result files) is verified working.
  </done>
</task>

</tasks>

<verification>
1. `scripts/run_cooperbench.sh` exists and `--dry-run` shows correct commands
2. Smoke test produced at least 5 result.json files under `repos/CooperBench/logs/command-a-solo/`
3. Result.json files contain valid JSON with status, agent, and cost fields
4. No orphaned Docker containers from the smoke test
</verification>

<success_criteria>
- Orchestrator script created with retry logic (3 attempts), infra_error tagging, and cost reporting
- Smoke test of 5 pairs completed end-to-end with valid results
- Pipeline validated: CLI -> Docker sandbox -> agent -> Command A -> result files
</success_criteria>

<output>
After completion, create `.planning/phases/01-execution-infrastructure/01-02-SUMMARY.md`
</output>
