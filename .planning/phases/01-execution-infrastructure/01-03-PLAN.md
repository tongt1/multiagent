---
phase: 01-execution-infrastructure
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified: []
autonomous: true

must_haves:
  truths:
    - "Solo mode run on full lite subset (100 pairs) completed with results in logs directory"
    - "Coop-with-comm run on full lite subset (100 pairs) completed with results in logs directory"
    - "Coop-no-comm run on full lite subset (100 pairs) completed with results in logs directory"
    - "Infrastructure failures are retried up to 3 times and tagged as infra_error"
    - "Cumulative API cost is tracked and reported per run"
  artifacts:
    - path: "repos/CooperBench/logs/command-a-solo/"
      provides: "Solo mode results for 100 pairs"
    - path: "repos/CooperBench/logs/command-a-coop-comm/"
      provides: "Coop with communication results for 100 pairs"
    - path: "repos/CooperBench/logs/command-a-coop-nocomm/"
      provides: "Coop without communication results for 100 pairs"
  key_links:
    - from: "scripts/run_cooperbench.sh"
      to: "repos/CooperBench/logs/"
      via: "cooperbench run producing result.json files"
      pattern: "result\\.json"
---

<objective>
Execute the full benchmark runs across all 3 experimental settings (solo, coop-with-comm, coop-no-comm) on the complete lite subset of 100 pairs each, using the orchestrator script with retry logic.

Purpose: This produces the raw experimental data that all downstream phases (data collection, analysis, figure generation) depend on. The three settings are the core of the CooperBench reproduction.
Output: Complete result logs for 300 task-pair executions (100 pairs x 3 settings) with cost tracking and infra_error tagging.
</objective>

<execution_context>
@/home/terry_tong_cohere_com/.claude/get-shit-done/workflows/execute-plan.md
@/home/terry_tong_cohere_com/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-execution-infrastructure/01-RESEARCH.md
@.planning/phases/01-execution-infrastructure/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run solo mode on full lite subset (100 pairs)</name>
  <files></files>
  <action>
Run the solo setting using the orchestrator script:

```bash
bash scripts/run_cooperbench.sh --setting solo
```

This runs `cooperbench run -n command-a-solo -m command-a-03-2025 -a mini_swe_agent -s lite --setting solo --backend docker -c 4 --no-auto-eval` with up to 3 retry attempts.

**Solo runs first** because:
1. It's the fastest (1 agent per pair vs 2 for coop)
2. Solo results are needed for difficulty score computation: d(t) = 1 - Solo(t)
3. It validates the pipeline at scale before committing to coop runs

**Expected duration:** 100 pairs at ~10 minutes each with concurrency 4 = ~250 minutes (~4 hours).

**Monitoring during execution:**
- Watch for systematic failures (all tasks failing = config issue, stop and fix)
- Check `repos/CooperBench/logs/command-a-solo/` for result.json files appearing
- Monitor Docker container count: `docker ps | grep cooperbench | wc -l` should show ~4 at a time

**After completion:**
- Count results: `find repos/CooperBench/logs/command-a-solo/ -name result.json | wc -l`
- Count by status: check how many pass, fail, Error
- Run the orchestrator's post-processing to tag infra_errors and report costs
- If many tasks are in Error state after 3 attempts, investigate the error patterns before proceeding to coop runs

**Resume support:** If execution is interrupted, re-running the same command skips completed tasks and retries Error tasks (built into upstream CLI).
  </action>
  <verify>
Count result.json files: `find repos/CooperBench/logs/command-a-solo/ -name result.json | wc -l` should be close to 100. Check for pass/fail/error distribution by reading status fields from result.json files. Verify cost data is present in results.
  </verify>
  <done>
Solo mode run completed on full lite subset. Approximately 100 result.json files exist under `repos/CooperBench/logs/command-a-solo/` with pass/fail/infra_error statuses tagged. Cost is tracked and reported.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run coop-with-comm and coop-no-comm on full lite subset (100 pairs each)</name>
  <files></files>
  <action>
Run the remaining two coop settings sequentially using the orchestrator:

**Step 1: Coop with communication**
```bash
bash scripts/run_cooperbench.sh --setting coop-comm
```

This runs `cooperbench run -n command-a-coop-comm -m command-a-03-2025 -a mini_swe_agent -s lite --setting coop --backend docker -c 4 --no-auto-eval` with up to 3 retry attempts.

Coop mode uses Redis for inter-agent messaging. The upstream CLI auto-starts Redis via `docker run redis:alpine` if not running (see `infra/redis.py`). No manual Redis setup needed.

**Expected duration:** 100 pairs, 2 agents per pair, concurrency 4 = ~500 minutes (~8 hours).

**After coop-comm completes:**
- Count results: `find repos/CooperBench/logs/command-a-coop-comm/ -name result.json | wc -l`
- Verify conversation.json files exist (inter-agent messages â€” critical for Figure 5 and 6 analysis)
- Check cost and error distribution

**Step 2: Coop without communication**
```bash
bash scripts/run_cooperbench.sh --setting coop-nocomm
```

This runs `cooperbench run -n command-a-coop-nocomm -m command-a-03-2025 -a mini_swe_agent -s lite --setting coop --no-messaging --backend docker -c 4 --no-auto-eval` with up to 3 retry attempts.

**Expected duration:** ~500 minutes (~8 hours).

**After coop-nocomm completes:**
- Count results: `find repos/CooperBench/logs/command-a-coop-nocomm/ -name result.json | wc -l`
- Verify no conversation.json files exist (no messaging in this setting)

**Final post-processing:**
After all 3 settings complete, run the orchestrator's post-processing:
- infra_error tagging across all runs
- Per-setting and total cost summary
- Overall status distribution: pass/fail/infra_error per setting

Print a summary table:
```
| Setting    | Total | Pass | Fail | Infra Error | Cost    |
|------------|-------|------|------|-------------|---------|
| solo       | 100   | ...  | ...  | ...         | $...    |
| coop-comm  | 100   | ...  | ...  | ...         | $...    |
| coop-nocomm| 100   | ...  | ...  | ...         | $...    |
| TOTAL      | 300   | ...  | ...  | ...         | $...    |
```
  </action>
  <verify>
Count result.json files for each setting:
- `find repos/CooperBench/logs/command-a-coop-comm/ -name result.json | wc -l` should be ~100
- `find repos/CooperBench/logs/command-a-coop-nocomm/ -name result.json | wc -l` should be ~100
Verify conversation.json exists in coop-comm logs but NOT in coop-nocomm logs. Check the final cost and status summary.
  </verify>
  <done>
All 3 benchmark settings completed on the full lite subset (300 total task-pair executions). Results are in `repos/CooperBench/logs/` with run names `command-a-solo`, `command-a-coop-comm`, and `command-a-coop-nocomm`. Infrastructure errors are tagged as `infra_error`. Cumulative API cost is tracked and reported per setting.
  </done>
</task>

</tasks>

<verification>
1. Solo: `find repos/CooperBench/logs/command-a-solo/ -name result.json | wc -l` is ~100
2. Coop-comm: `find repos/CooperBench/logs/command-a-coop-comm/ -name result.json | wc -l` is ~100
3. Coop-nocomm: `find repos/CooperBench/logs/command-a-coop-nocomm/ -name result.json | wc -l` is ~100
4. Coop-comm has conversation.json files; coop-nocomm does not
5. infra_error tagging applied to Error results matching infrastructure patterns
6. Cost summary printed per setting and total
</verification>

<success_criteria>
- All 3 experimental settings (solo, coop-comm, coop-nocomm) completed on 100 pairs each
- Results written to logs directory with pass/fail/infra_error status tags
- Infrastructure failures retried up to 3 times
- Cumulative API cost tracked and reported
- Data ready for Phase 2 (Results Collection and Data Foundation)
</success_criteria>

<output>
After completion, create `.planning/phases/01-execution-infrastructure/01-03-SUMMARY.md`
</output>
