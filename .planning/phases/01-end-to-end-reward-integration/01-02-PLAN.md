---
phase: 01-end-to-end-reward-integration
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - tests/test_reward_shaping_integration.py
autonomous: true

must_haves:
  truths:
    - "Running identity strategy produces numerically identical item.data['rewards'] as running with no strategy configured (np.allclose with atol=1e-6)"
    - "Running a non-identity strategy (coma_advantage) produces shaped rewards that differ from raw rewards on at least one sample input"
    - "A multi-step integration test (5 simulated training steps) shows identity and no-strategy produce identical reward sequences while a non-identity strategy produces different reward sequences"
    - "A gradient-norm comparison test uses torch to compute loss.backward() on shaped vs unshaped rewards (5 steps), confirming gradient norms differ for non-identity strategies and match for identity"
    - "A lightweight CI variant of the gradient-norm test runs in 1-2 steps with @pytest.mark.ci marker"
    - "DebateMetricStreamerConfig with an invalid strategy name raises an error during __init__() construction of DebateMetricStreamer (which IS config load time, since __init__ calls create_reward_strategy()), not at runtime during get()"
    - "Existing configs with no reward_shaping fields produce identical behavior to explicit identity (backward compatible)"
  artifacts:
    - path: "tests/test_reward_shaping_integration.py"
      provides: "Identity regression tests and non-identity differentiation tests"
      contains: "def test_identity_regression"
    - path: "tests/test_reward_shaping_integration.py"
      provides: "Gradient-norm simulation using torch"
      contains: "def test_gradient_norm_differs_with_shaping"
    - path: "tests/test_reward_shaping_integration.py"
      provides: "Lightweight CI gradient check"
      contains: "pytest.mark.ci"
    - path: "tests/test_reward_shaping_integration.py"
      provides: "Invalid strategy __init__ failure test"
      contains: "def test_invalid_strategy_name_fails_at_init"
  key_links:
    - from: "tests/test_reward_shaping_integration.py"
      to: "src/training/wandb_enrichment/debate_streamer.py"
      via: "DebateMetricStreamer.get() called multiple times, item.data['rewards'] compared across strategy configs"
      pattern: "np\\.allclose.*atol"
    - from: "tests/test_reward_shaping_integration.py"
      to: "torch"
      via: "torch.tensor wrapping rewards, loss.backward() computing gradients, grad.norm() compared"
      pattern: "torch\\.tensor.*backward.*grad"
---

<objective>
Verify identity regression (identity strategy == no strategy configured) and confirm non-identity strategies produce observably different shaped rewards, proving the gradient path is live. Includes a torch-based gradient-norm comparison and a lightweight CI variant.

Purpose: The identity regression check (RINT-04) ensures backward compatibility -- existing training configs produce identical behavior. The non-identity differentiation test proves shaped rewards actually affect the values the learner sees. The gradient-norm test (per locked decision R12/R13) uses torch.backward() to prove shaped rewards produce different gradient norms than unshaped, confirming end-to-end gradient path liveness. The invalid strategy test confirms fail-fast behavior at init time (per locked decision).

Output: Integration tests in test_reward_shaping_integration.py demonstrating identity regression within float tolerance, non-identity differentiation, gradient-norm divergence, CI-compatible lightweight check, and fail-fast config validation.
</objective>

<execution_context>
@/home/terry_tong_cohere_com/.claude/get-shit-done/workflows/execute-plan.md
@/home/terry_tong_cohere_com/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-end-to-end-reward-integration/01-CONTEXT.md
@.planning/phases/01-end-to-end-reward-integration/01-RESEARCH.md
@.planning/phases/01-end-to-end-reward-integration/01-01-SUMMARY.md

Key source files:
@src/training/wandb_enrichment/debate_streamer.py
@tests/test_reward_shaping_integration.py
@tests/test_debate_streamer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Identity regression, non-identity differentiation, and gradient-norm tests</name>
  <files>tests/test_reward_shaping_integration.py</files>
  <action>
Add a new test class `TestIdentityRegressionAndGradientPath` to `tests/test_reward_shaping_integration.py` (the sole file modified in this task). These tests depend on the reward mutation from Plan 01-01 being in place.

**Helper function** (add at class level or module level):

```python
def _run_streamer_and_collect_rewards(items, config_kwargs):
    """Run DebateMetricStreamer.get() and return post-mutation reward values.

    Creates fresh MockActorOutputItem copies (to avoid mutation contamination),
    runs streamer.get(), and returns the mutated item.data["rewards"] values
    as a numpy array.

    Args:
        items: List of (reward, role_label) tuples to create MockActorOutputItems from.
        config_kwargs: Dict of DebateMetricStreamerConfig kwargs.

    Returns:
        np.ndarray of shape (len(items),) with post-mutation reward values.
    """
    mock_items = [MockActorOutputItem(reward=r, role_label=rl) for r, rl in items]
    upstream = MockUpstreamStreamer(mock_items, {})
    config = DebateMetricStreamerConfig(**config_kwargs)
    streamer = DebateMetricStreamer(config, upstream, MagicMock())
    result_items, _ = streamer.get()
    return np.array([item.data["rewards"].item() for item in result_items])
```

**Tests to add:**

1. `test_identity_regression_single_step()`:
   - Define items: 8 items, all solver, rewards=[5.0, 0.0, 5.0, 0.0, 5.0, 5.0, 0.0, 0.0].
   - Run with `config_kwargs={"n_rollouts_per_prompt": 8}` (no reward_shaping_strategy -- defaults to identity).
   - Run with `config_kwargs={"n_rollouts_per_prompt": 8, "reward_shaping_strategy": "identity"}` (explicit identity).
   - Assert `np.allclose(rewards_no_strategy, rewards_identity, atol=1e-6, rtol=1e-5)`.
   - Also assert both are `np.allclose` to the original raw rewards `[5.0, 0.0, 5.0, 0.0, 5.0, 5.0, 0.0, 0.0]`.
   - This is the core RINT-04 regression check.

2. `test_identity_regression_with_mixed_roles()`:
   - Items: 4 solver (rewards 5.0, 0.0, 5.0, 0.0), 4 verifier (rewards 0.0, 5.0, 0.0, 5.0).
   - Run no-strategy and explicit identity.
   - Assert `np.allclose` between both runs.
   - Assert both match original raw rewards.
   - Proves identity regression holds with mixed role labels.

3. `test_multi_step_identity_vs_no_strategy()`:
   - Simulate 5 training steps. Each step:
     - Create fresh items with slightly different rewards (use `np.random.RandomState(seed=step)` for reproducibility).
     - Run no-strategy streamer, collect rewards.
     - Run explicit identity streamer, collect rewards.
     - Assert `np.allclose(no_strategy_rewards, identity_rewards, atol=1e-6)` at each step.
   - This is the multi-step regression test from the user decision (5-10 steps, using 5 for speed).

4. `test_non_identity_produces_different_rewards()`:
   - Items: 8 items, all solver, rewards=[5.0, 0.0, 5.0, 0.0, 5.0, 5.0, 0.0, 0.0].
   - Run with identity (no strategy).
   - Run with coma_advantage (n_rollouts_per_prompt=8).
   - COMA advantage with mean=2.5 produces [2.5, -2.5, 2.5, -2.5, 2.5, 2.5, -2.5, -2.5].
   - Assert `not np.allclose(identity_rewards, coma_rewards)`.
   - Assert `np.allclose(coma_rewards, [2.5, -2.5, 2.5, -2.5, 2.5, 2.5, -2.5, -2.5], atol=1e-6)`.
   - This proves shaped rewards are NOT just logged -- they actually change item.data["rewards"].

5. `test_multi_step_non_identity_differs_from_identity()`:
   - Simulate 5 training steps. Each step:
     - Create items with random rewards.
     - Run identity, collect rewards.
     - Run coma_advantage, collect rewards.
     - Assert `not np.allclose(identity_rewards, coma_rewards)` for at least one step.
   - Track cumulative sum of |identity - coma| across steps.
   - Assert cumulative difference > 0 (shaped rewards diverge from raw over multiple steps).

6. `test_gradient_norm_differs_with_shaping()`:
   - **This test uses torch to simulate gradient computation (per locked decision R12/R13).**
   - Import torch at the top of the test (skip test if torch not available: `pytest.importorskip("torch")`).
   - Simulate 5 training steps. Each step:
     a. Create items with random rewards (seeded).
     b. Run identity streamer -> collect raw_rewards as np.ndarray.
     c. Run coma_advantage streamer -> collect shaped_rewards as np.ndarray.
     d. Create a simple mock "model parameter": `param = torch.randn(8, requires_grad=True)`.
     e. Compute loss_raw = `(torch.tensor(raw_rewards, dtype=torch.float32) * param).sum()`.
     f. Compute loss_shaped = `(torch.tensor(shaped_rewards, dtype=torch.float32) * param.detach().clone().requires_grad_(True)).sum()`.
     g. Call `loss_raw.backward()` and `loss_shaped.backward()`.
     h. Record `grad_norm_raw = param.grad.norm().item()` and `grad_norm_shaped` similarly.
   - After 5 steps, assert that gradient norms differ in at least one step: `any(abs(raw - shaped) > 1e-6 for raw, shaped in zip(raw_norms, shaped_norms))`.
   - Also run same test with identity vs no-strategy: assert gradient norms are identical across all 5 steps.
   - This directly verifies the locked decision: "gradient-norm comparison (5-10 steps)."

7. `test_gradient_norm_ci_lightweight()`:
   - **Marked with `@pytest.mark.ci` for CI-only execution (per locked decision R13).**
   - Same structure as test 6 but with only 2 steps (1-2 steps per locked decision).
   - Use fixed (non-random) rewards for deterministic behavior in CI.
   - Assert gradient norms differ between identity and coma_advantage for at least 1 of 2 steps.
   - This is the lightweight CI check.

8. `test_potential_based_produces_different_rewards()`:
   - Items: 4 items, rewards=[5.0, 0.0, 5.0, 0.0], all solver.
   - Test at strategy level: create PotentialBasedShaper directly, call shape_rewards with metadata containing state_start and state_end, verify output differs from input.
   - This verifies the potential_based path can produce different values. The write-back wiring is already proven by the per-role and global tests above.

All tests use `MagicMock()` for metrics_collector. Use existing MockActorOutputItem and MockUpstreamStreamer from the same file.
  </action>
  <verify>
Run: `cd /home/terry_tong_cohere_com/reward-training && python -m pytest tests/test_reward_shaping_integration.py::TestIdentityRegressionAndGradientPath -x -v`
All 8 tests pass. Then run full suite: `python -m pytest tests/test_reward_shaping_integration.py -x -v` to confirm no regressions.
  </verify>
  <done>
8 tests verify: (a) single-step identity regression (no-strategy == identity, both match raw), (b) identity regression with mixed roles, (c) multi-step identity regression over 5 steps, (d) COMA advantage produces different rewards than identity, (e) multi-step divergence between identity and non-identity accumulates, (f) gradient-norm comparison over 5 steps using torch.backward() proves shaped rewards produce different gradients, (g) lightweight CI gradient-norm check in 2 steps with @pytest.mark.ci, (h) potential_based strategy can produce different shaped values. Identity regression confirmed with atol=1e-6, rtol=1e-5 tolerance per user decision.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add backward compatibility and config validation tests</name>
  <files>tests/test_reward_shaping_integration.py</files>
  <action>
Add a new test class `TestBackwardCompatibilityAndConfigValidation` to `tests/test_reward_shaping_integration.py` with tests for the unconfigured fallback behavior and invalid config detection specified in the user decisions.

**Tests to add:**

1. `test_no_config_backward_compatible()`:
   - Create DebateMetricStreamerConfig with NO reward_shaping_strategy field set (rely on default "").
   - Create streamer, run get() with 4 solver items.
   - Verify item.data["rewards"] are unchanged (identity passthrough).
   - Verify streamer._reward_shaper.name == "identity".
   - This confirms: "existing configs with no reward_shaping fields work exactly as before."

2. `test_empty_string_strategy_uses_identity()`:
   - Config with reward_shaping_strategy="" explicitly.
   - Verify identity behavior (already tested elsewhere but this is explicit backward compat).

3. `test_invalid_strategy_name_fails_at_init()`:
   - **This test verifies the locked decision: "Invalid strategy name: error at config load time (fail fast)."**
   - **Note:** "Config load time" = `DebateMetricStreamer.__init__()` construction time, because `__init__()` calls `create_reward_strategy()` which resolves the strategy name from the registry. This IS when config is loaded and validated.
   - Create DebateMetricStreamerConfig with reward_shaping_strategy="nonexistent_strategy".
   - Assert that calling `DebateMetricStreamer(config, upstream, metrics_collector)` raises an error (KeyError or ValueError) during `__init__()` construction.
   - **Critically: the error must occur during __init__(), NOT during a later get() call.** To prove this, wrap ONLY the constructor call in `pytest.raises`:
     ```python
     config = DebateMetricStreamerConfig(reward_shaping_strategy="nonexistent_strategy", ...)
     upstream = MockUpstreamStreamer([], {})
     with pytest.raises((KeyError, ValueError), match="nonexistent_strategy"):
         DebateMetricStreamer(config, upstream, MagicMock())
     # get() is never called -- error is at init time
     ```
   - Use `pytest.raises((KeyError, ValueError), match="nonexistent_strategy")` to match either exception type from the registry.

4. `test_info_log_on_identity_default()`:
   - Config with no reward_shaping_strategy (defaults to identity).
   - Use `caplog` fixture or mock logger to verify an INFO log message is emitted containing "identity" during __init__.
   - The existing code already logs: "DebateMetricStreamer: initialized reward shaping strategy 'identity'". Verify this message appears.

5. `test_shaped_rewards_dtype_preserved()`:
   - Create items with float32 rewards (np.array(5.0, dtype=np.float32)).
   - Run with coma_advantage strategy.
   - After get(), verify item.data["rewards"].dtype == np.float32.
   - This ensures the write-back preserves original dtype per implementation spec.

All tests use existing mock infrastructure (MockActorOutputItem, MockUpstreamStreamer, MagicMock).
  </action>
  <verify>
Run: `cd /home/terry_tong_cohere_com/reward-training && python -m pytest tests/test_reward_shaping_integration.py::TestBackwardCompatibilityAndConfigValidation -x -v`
All 5 tests pass. Then run full suite: `python -m pytest tests/ -x -v --ignore=tests/test_streamlit_app.py` to confirm no regressions across all test files.
  </verify>
  <done>
5 tests verify: (a) no-config backward compatibility, (b) empty string uses identity, (c) invalid strategy fails at __init__() time with error matching the invalid name (not at runtime during get()), (d) INFO log emitted on identity default, (e) dtype preserved through write-back. Combined with Task 1, all RINT-04 requirements, locked decisions about regression verification (R12/R13 gradient-norm, CI check), and unconfigured fallback behavior are covered.
  </done>
</task>

</tasks>

<verification>
1. Identity regression: `test_identity_regression_single_step` and `test_identity_regression_with_mixed_roles` both pass with atol=1e-6.
2. Multi-step regression: `test_multi_step_identity_vs_no_strategy` passes for 5 steps.
3. Non-identity differentiation: `test_non_identity_produces_different_rewards` shows COMA advantage values differ from raw.
4. Multi-step divergence: `test_multi_step_non_identity_differs_from_identity` accumulates measurable difference.
5. Gradient-norm comparison: `test_gradient_norm_differs_with_shaping` uses torch.backward() over 5 steps to prove gradient norms differ.
6. CI lightweight check: `test_gradient_norm_ci_lightweight` runs in 2 steps with @pytest.mark.ci.
7. Backward compat: `test_no_config_backward_compatible` and `test_empty_string_strategy_uses_identity` pass.
8. Fail fast at init: `test_invalid_strategy_name_fails_at_init` catches error during __init__() constructor, not during get().
9. Full test suite green: `python -m pytest tests/test_reward_shaping_integration.py tests/test_debate_streamer.py tests/test_reward_shaping.py tests/test_reward_shaping_registry.py -v`
</verification>

<success_criteria>
- Identity strategy produces item.data["rewards"] values within atol=1e-6 of no-strategy baseline
- COMA advantage produces item.data["rewards"] values that measurably differ from identity
- Multi-step simulation (5 steps) confirms identity regression holds across steps
- Multi-step simulation confirms non-identity divergence accumulates
- Gradient-norm comparison (5 steps, torch.backward()) confirms shaped rewards produce different gradient norms than unshaped
- Lightweight CI gradient-norm check (2 steps, @pytest.mark.ci) passes
- Invalid strategy name raises error at __init__() time (fail fast, not runtime)
- No-config and empty-string configs default to identity (backward compatible)
- All 13 new tests pass, all existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-end-to-end-reward-integration/01-02-SUMMARY.md`
</output>
