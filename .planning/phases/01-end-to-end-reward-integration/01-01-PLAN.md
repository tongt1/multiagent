---
phase: 01-end-to-end-reward-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/training/wandb_enrichment/debate_streamer.py
  - tests/test_debate_streamer.py
  - tests/test_reward_shaping_integration.py
autonomous: true

must_haves:
  truths:
    - "After DebateMetricStreamer.get() returns, every item's data['rewards'] contains shaped values (not raw correctness scores) when a shaping strategy is configured -- and the downstream FlinkRlooLearner reads these mutated item.data['rewards'] for GRPO advantage computation, so shaping directly affects training gradients"
    - "A solver item in a per-role shaping run receives a shaped reward that differs from its raw correctness score (e.g., COMA advantage produces mean-centered values), while a judge item always receives 0.0 reward regardless of strategy"
    - "A solver item in a global shaping run receives the strategy's output value (e.g., potential-based shaped value), while a judge item always receives 0.0 reward regardless of strategy"
    - "Unshaped debate metrics (debate/reward/solver, etc.) equal values computed from original raw rewards, unaffected by mutation"
    - "If a per-role strategy does not produce a reward for a role (and role is not judge), the raw correctness score is preserved as fallback"
  artifacts:
    - path: "src/training/wandb_enrichment/debate_streamer.py"
      provides: "In-place reward mutation in get() method"
      contains: "item.data.*rewards.*=.*shaped"
    - path: "tests/test_debate_streamer.py"
      provides: "Tests verifying item.data['rewards'] mutation"
      contains: "def test_shaped_rewards_written_to_items"
    - path: "tests/test_reward_shaping_integration.py"
      provides: "Integration test verifying mutation with each strategy type"
      contains: "def test_.*_mutates_item_rewards"
  key_links:
    - from: "src/training/wandb_enrichment/debate_streamer.py"
      to: "item.data['rewards']"
      via: "shaped reward write-back in get() after _compute_shaped_reward_metrics"
      pattern: "item\\.data\\[.rewards.\\].*=.*shaped"
    - from: "src/training/wandb_enrichment/debate_streamer.py"
      to: "src/training/reward_shaping/base.py"
      via: "shape_rewards() call producing shaped values that replace raw rewards"
      pattern: "self\\._reward_shaper\\.shape_rewards"
---

<objective>
Wire shaped rewards from reward shaping strategies back into item.data["rewards"] so the downstream GRPO/RLOO learner receives shaped values for gradient computation -- not just raw correctness scores.

Purpose: Close the critical gap where shaped rewards are computed and logged to WandB but never replace the raw rewards consumed by the learner. Without this, all 5 reward shaping strategies are cosmetic (metrics-only) and have zero effect on training.

Output: Modified DebateMetricStreamer that mutates item.data["rewards"] with shaped values after computing unshaped metrics; unit and integration tests proving the mutation occurs correctly for both per-role dict and global ndarray strategy return types.
</objective>

<execution_context>
@/home/terry_tong_cohere_com/.claude/get-shit-done/workflows/execute-plan.md
@/home/terry_tong_cohere_com/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-end-to-end-reward-integration/01-CONTEXT.md
@.planning/phases/01-end-to-end-reward-integration/01-RESEARCH.md

Key source files (read these first):
@src/training/wandb_enrichment/debate_streamer.py
@src/training/reward_shaping/base.py
@src/training/reward_shaping/registry.py
@src/training/reward_shaping/identity.py
@src/training/reward_shaping/difference_rewards.py
@tests/test_debate_streamer.py
@tests/test_reward_shaping_integration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add shaped reward write-back to DebateMetricStreamer.get()</name>
  <files>src/training/wandb_enrichment/debate_streamer.py</files>
  <action>
Modify `_DebateMetricStreamerImpl.get()` to write shaped rewards back into `item.data["rewards"]` after computing unshaped metrics but before returning. The critical requirement is that unshaped metrics (debate/reward/solver, debate/frac_zero_std, etc.) are computed from the ORIGINAL raw rewards, and the mutation happens AFTER that computation.

**Why this mutation matters (learner data flow):** The Flink pipeline calls `streamer.get()` which returns `(items, metrics)`. These items are then fed directly into `FlinkRlooLearner`, which reads `item.data["rewards"]` for GRPO loss advantage computation. By mutating `item.data["rewards"]` in-place before returning from `get()`, the learner receives shaped values for gradient computation. Add a code comment at the write-back site: `# items from get() feed into FlinkRlooLearner -> GRPO loss reads item.data["rewards"] for advantage computation`.

**Execution order inside get() -- follow this exactly:**

```
STEP A: Extract rewards array and role_labels from items (existing code, ~lines 170-185)
STEP B: Capture original_dtype = items[0].data["rewards"].dtype
STEP C: Compute unshaped debate metrics (existing code, ~lines 191-200)
         - compute_per_role_rewards(rewards, role_labels)
         - compute_zero_advantage_metrics(rewards, effective_n_rollouts)
         These use the ORIGINAL raw rewards array. Do NOT move these.
STEP D: Compute shaped rewards and build per-item array (NEW -- details below)
STEP E: Write shaped values back into items (NEW -- details below)
STEP F: Log rollout table (existing code, ~line 213) -- now sees shaped rewards
STEP G: Return (items, metrics) as before
```

**STEP D detail -- refactor _compute_shaped_reward_metrics:**

Rename to `_compute_and_apply_shaped_rewards`. Change return type from `dict` to `Tuple[dict, np.ndarray]`. The second element is `shaped_per_item`, an np.ndarray of shape `(B,)` where B = len(items).

After computing `shaped = self._reward_shaper.shape_rewards(rewards, role_labels, ...)`:

**Case 1: `shaped` is a `dict[str, np.ndarray(B,)]` (per-role strategies):**

**Per-role contract:** All registered per-role strategies (difference_rewards, coma_advantage, reward_mixing) populate all B indices for all roles they return. That is, `shaped[role]` is always an ndarray of length B where every index is meaningful. This contract is verified in `difference_rewards.py`, `coma_advantage.py`, and `reward_mixing.py`. Add a code comment documenting this contract at the dict unpacking site.

```python
shaped_per_item = np.copy(rewards)  # start from raw as default fallback
# Contract: all per-role strategies populate all B indices for every role key
# (verified in difference_rewards.py, coma_advantage.py, reward_mixing.py)
for idx in range(len(items)):
    role = role_labels[idx]
    if role == "judge":
        shaped_per_item[idx] = 0.0
    elif role in shaped:
        shaped_per_item[idx] = shaped[role][idx]
    # else: keep rewards[idx] (raw fallback -- per locked decision)
```

The key mapping is: `for item i with role r â†’ item.data['rewards'] = shaped_dict[r][i]`. Each role key in the dict maps to an ndarray of length B (batch size). The value at index i is the shaped reward for the i-th item in the batch. Items whose role is not a key in the dict retain their raw reward.

**Case 2: `shaped` is an `np.ndarray(B,)` (global strategies):**

```python
shaped_per_item = np.copy(shaped)
for idx in range(len(items)):
    if role_labels[idx] == "judge":
        shaped_per_item[idx] = 0.0
```

Return `(metrics_dict, shaped_per_item)`.

**STEP E detail -- write-back loop:**

```python
for idx, item in enumerate(items):
    item.data["rewards"] = np.array(shaped_per_item[idx], dtype=original_dtype)
```

This preserves the original dtype (e.g., float32) to prevent unintended dtype promotion.

**STEP D call site in get():**

```python
# After STEP C (unshaped metrics already computed):
try:
    shaped_metrics, shaped_per_item = self._compute_and_apply_shaped_rewards(rewards, role_labels)
    debate_metrics.update(shaped_metrics)
    # STEP E: Write shaped rewards back into items for learner consumption
    for idx, item in enumerate(items):
        item.data["rewards"] = np.array(shaped_per_item[idx], dtype=original_dtype)
except Exception as e:
    logger.warning(f"DebateMetricStreamer: reward shaping failed: {e}")
```

**Logging:** Add a one-time INFO log when reward mutation is active (non-identity strategy): `"DebateMetricStreamer: shaped rewards written to item.data['rewards'] using strategy '{name}'"`. Use an instance flag `self._logged_mutation = False` to log only on the first get() call. **Preserve the existing `logger.info` line** in `__init__()` that logs the initialized strategy name -- do not remove or modify it.

**Constraints:**
- Do NOT modify the return signature of get(). It still returns (items, metrics).
- Do NOT change the metrics dict format -- all existing debate/shaped_reward/* metrics remain.
- The mutation MUST happen AFTER unshaped metrics computation (STEP C) and BEFORE rollout table logging (STEP F).
  </action>
  <verify>
Run: `cd /home/terry_tong_cohere_com/reward-training && python -m pytest tests/test_debate_streamer.py -x -v`
All existing tests must pass (no regression). The mutation is invisible to existing tests because they don't check item.data["rewards"] post-get().
  </verify>
  <done>
DebateMetricStreamer.get() writes shaped reward values into item.data["rewards"] for every item before returning. Unshaped debate metrics remain computed from original raw rewards. The refactored _compute_and_apply_shaped_rewards method returns both metrics and per-item shaped values. Per-role dict results use the mapping: for item i with role r, shaped_per_item[i] = shaped_dict[r][i] (with judge=0.0, missing-role=raw fallback). Global ndarray results index directly with judge override.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit and integration tests for reward mutation</name>
  <files>tests/test_debate_streamer.py, tests/test_reward_shaping_integration.py</files>
  <action>
Add tests proving that item.data["rewards"] is mutated with shaped values after DebateMetricStreamer.get(). Tests cover both per-role dict strategies and global ndarray strategies.

**In tests/test_debate_streamer.py, add:**

1. `test_shaped_rewards_written_to_items_identity()`:
   - Create 4 items with reward=5.0 (solver) and reward=0.0 (solver).
   - Config with no reward_shaping_strategy (identity default).
   - After get(), verify item.data["rewards"] values are UNCHANGED (identity is passthrough).
   - Use `np.isclose(item.data["rewards"].item(), expected)` for comparison.

2. `test_shaped_rewards_written_to_items_global_strategy()`:
   - Create 2 items with rewards [5.0, 0.0], role_labels ["solver", "solver"].
   - Config with reward_shaping_strategy="potential_based", params={"gamma": 0.99, "potential_type": "zero"}.
   - After get(), verify item.data["rewards"] values are the potential_based shaped values (with zero potential, they should equal originals since gamma*0 - 0 = 0).
   - This proves the write-back path works for global ndarray strategies.

3. `test_shaped_rewards_written_to_items_per_role_strategy()`:
   - Create 4 items: 2 solver (rewards 5.0, 0.0), 2 verifier (rewards 5.0, 0.0).
   - Config with reward_shaping_strategy="coma_advantage", params={"n_rollouts_per_prompt": 4}.
   - After get(), verify item.data["rewards"] values differ from originals.
   - COMA with 4 rollouts per prompt, mean=2.5, so advantages=[2.5, -2.5, 2.5, -2.5].
   - Solver items (indices 0,1) should get solver advantage from the dict.
   - Verifier items (indices 2,3) should get verifier advantage from the dict.
   - Use `pytest.approx()` for float comparison.

4. `test_judge_items_get_zero_reward()`:
   - Create 3 items: solver (5.0), verifier (5.0), judge (5.0).
   - Config with reward_shaping_strategy="difference_rewards".
   - After get(), verify judge item's data["rewards"] == 0.0.
   - Solver and verifier items should have their respective shaped values (with no metadata, difference_rewards falls back to raw for all roles, so solver=5.0, verifier=5.0).

5. `test_unshaped_metrics_unchanged_after_mutation()`:
   - Create items and run get() with a non-identity strategy.
   - Verify debate/reward/solver metric equals the ORIGINAL raw mean, not the shaped mean.
   - This is the critical regression test ensuring unshaped metrics use pre-mutation values.

6. `test_reward_mutation_fallback_on_missing_role()`:
   - Create items with role_label="custom_role" (not solver/verifier/judge).
   - Config with reward_shaping_strategy="difference_rewards".
   - After get(), verify items with unknown role fall back to raw reward value (per user decision).

**In tests/test_reward_shaping_integration.py, add to TestDebateMetricStreamerRewardShaping class:**

7. `test_reward_mixing_mutates_item_rewards()`:
   - 4 items: 2 solver, 2 verifier with known rewards.
   - Config with reward_mixing alpha=0.5.
   - After get(), check that solver items have solver's mixed reward and verifier items have verifier's mixed reward.
   - Since reward_mixing falls back to G for local when no metadata: r = 0.5*G + 0.5*G = G. So shaped equals raw. Use alpha=0.0 with metadata to see different values, or just verify write-back happens (items changed type-safely).

8. `test_difference_rewards_mutates_item_rewards()`:
   - 4 items all solver, known rewards.
   - Config with difference_rewards.
   - After get(), verify each item's data["rewards"] is the solver's difference reward (with no metadata, fallback = raw reward copy).

For all tests: import MockActorOutputItem and MockUpstreamStreamer from the existing test helpers (they are defined at module level in each file).
  </action>
  <verify>
Run: `cd /home/terry_tong_cohere_com/reward-training && python -m pytest tests/test_debate_streamer.py tests/test_reward_shaping_integration.py -x -v`
All new tests pass. All existing tests still pass. Total test count increases by 8.
  </verify>
  <done>
8 new tests verify: (a) identity write-back is a no-op, (b) global strategy write-back works, (c) per-role strategy write-back correctly indexes by role_label, (d) judge items get zero reward, (e) unshaped metrics are unaffected by mutation, (f) missing role falls back to raw reward, (g) reward_mixing mutates items, (h) difference_rewards mutates items.
  </done>
</task>

</tasks>

<verification>
1. All existing tests in test_debate_streamer.py pass without modification (no regression).
2. All existing tests in test_reward_shaping_integration.py pass without modification.
3. New tests demonstrate item.data["rewards"] contains shaped values post-get().
4. New tests demonstrate unshaped metrics are unaffected by the mutation.
5. Run full test suite: `python -m pytest tests/test_debate_streamer.py tests/test_reward_shaping_integration.py tests/test_reward_shaping.py tests/test_reward_shaping_registry.py -v`
</verification>

<success_criteria>
- item.data["rewards"] is mutated with shaped reward values in DebateMetricStreamer.get()
- Per-role dict strategies (difference_rewards, coma_advantage, reward_mixing) use mapping: shaped_per_item[i] = shaped_dict[role_labels[i]][i]
- Global ndarray strategies (identity, potential_based) index directly: shaped_per_item[i] = shaped[i]
- Judge items receive zero reward in both dict and ndarray paths
- Missing role in per-role dict falls back to raw reward
- Unshaped debate metrics are computed from original raw rewards (not mutated values)
- All existing tests pass without modification
- 8 new tests pass covering all mutation paths
</success_criteria>

<output>
After completion, create `.planning/phases/01-end-to-end-reward-integration/01-01-SUMMARY.md`
</output>
