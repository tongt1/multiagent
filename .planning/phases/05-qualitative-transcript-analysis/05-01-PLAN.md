---
phase: 05-qualitative-transcript-analysis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/analyze_qualitative.py
  - data/qualitative_metrics.json
autonomous: true
requirements:
  - QUAL-01
  - QUAL-02
  - QUAL-03
  - QUAL-04

must_haves:
  truths:
    - "Plan:Question ratio is computed per trajectory with infinite ratios (zero-question trajectories) handled as None and reported separately via n_finite/n_infinite counts"
    - "Mann-Whitney U test correlates finite Plan:Question ratios with merge outcome and produces a p-value"
    - "First-turn planning (message index 0 classified as plan) is detected per trajectory and correlated with merge outcome via Fisher's exact test on a 2x2 contingency table"
    - "File path mentions and line number mentions are counted per trajectory using regex patterns"
    - "A summary comparison table shows all qualitative metrics for conflict (merge_failed) vs no-conflict (merge_clean) groups with statistical test results"
    - "Speech act classification reuses the exact classify_speech_act function imported from analyze_fig5.py for consistency with Phase 3"
  artifacts:
    - path: "scripts/analyze_qualitative.py"
      provides: "Qualitative transcript analysis script computing QUAL-01 through QUAL-04"
      contains: "from analyze_fig5 import classify_speech_act"
    - path: "data/qualitative_metrics.json"
      provides: "Per-trajectory metrics, group comparisons, and summary table with p-values"
      contains: "summary_table"
  key_links:
    - from: "scripts/analyze_qualitative.py"
      to: "scripts/analyze_fig5.py"
      via: "import classify_speech_act"
      pattern: "from analyze_fig5 import classify_speech_act"
    - from: "scripts/analyze_qualitative.py"
      to: "data/results.json"
      via: "json.load input"
      pattern: "json\\.load"
    - from: "scripts/analyze_qualitative.py"
      to: "data/qualitative_metrics.json"
      via: "json.dump output"
      pattern: "json\\.dump"
---

<objective>
Compute qualitative metrics from the 100 coop-comm transcripts and correlate them with merge conflict outcomes, producing a single JSON output file and formatted console summary table.

Purpose: Phase 5 (final phase) adds depth to the reproduction by analyzing structural patterns in agent communication — Plan:Question ratios, first-turn planning behavior, and specificity metrics — and testing whether these patterns correlate with cooperation outcomes. This completes the qualitative analysis layer beyond the quantitative Figures 4-6.

Output: `scripts/analyze_qualitative.py` (analysis script), `data/qualitative_metrics.json` (structured results with per-trajectory metrics, group comparisons, and summary table)
</objective>

<execution_context>
@/home/terry_tong_cohere_com/.claude/get-shit-done/workflows/execute-plan.md
@/home/terry_tong_cohere_com/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-qualitative-transcript-analysis/05-RESEARCH.md
@scripts/analyze_fig5.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create qualitative transcript analysis script</name>
  <files>scripts/analyze_qualitative.py</files>
  <action>
Create `scripts/analyze_qualitative.py` following the established analysis script pattern (argparse CLI, load results.json, compute metrics, write JSON, print summary). Use system Python (has scipy).

**Imports and setup:**
- Import `classify_speech_act` from `analyze_fig5` (add scripts dir to sys.path via `Path(__file__).resolve().parent`)
- Import `fisher_exact`, `mannwhitneyu` from `scipy.stats`
- Default input: `data/results.json`, default output: `data/qualitative_metrics.json`

**Specificity regex patterns (QUAL-03):**
- File paths: `re.compile(r'(?:[\w.-]+/)+[\w.-]+\.\w{1,5}')` for path-like patterns (e.g., `src/jinja2/loaders.py`)
- Standalone filenames: `re.compile(r'\b[\w.-]+\.(?:py|js|ts|tsx|jsx|go|rs|java|cpp|c|h|rb|yml|yaml|json|toml|cfg|txt|md|html|css|scss)\b')`
- Line numbers: list of compiled patterns — `r'\bline\s+\d+'` (IGNORECASE), `r'\blines?\s+\d+\s*[-\u2013to]+\s*\d+'` (IGNORECASE), `r'\bL\d+\b'`, `r'\bat\s+line\b'` (IGNORECASE)
- `count_file_mentions(text)` returns 1 if any file pattern matches the message, 0 otherwise (per-message, not per-match)
- `count_line_mentions(text)` returns 1 if any line number pattern matches, 0 otherwise

**Per-trajectory computation (`compute_trajectory_metrics`):**
- Filter records to `setting == "coop-comm"` only (100 records)
- For each trajectory, classify all messages via `classify_speech_act` and count plan/question/update/other
- Plan:Question ratio: `plans/questions` if questions > 0; `None` if questions == 0 and plans > 0; `0.0` if both are 0
- First-turn planning: `classify_speech_act(messages[0]["message"]) == "plan"` if messages exist, else False
- File mentions: sum of `count_file_mentions(msg["message"])` across all messages in trajectory
- Line mentions: sum of `count_line_mentions(msg["message"])` across all messages

**Group comparison and statistical tests:**
- Split trajectories by `merge_outcome` into `merge_clean` (n=59) and `merge_failed` (n=41) groups
- QUAL-01: Collect finite Plan:Question ratios per group. Run `mannwhitneyu(clean_ratios, failed_ratios, alternative='two-sided')`. Also report n_infinite per group.
- QUAL-02: Build 2x2 contingency table `[[plan_clean, plan_failed], [noplan_clean, noplan_failed]]`. Run `fisher_exact(table)`. Report conflict rates for plan-first vs no-plan-first groups.
- QUAL-03: Compute mean/median file mentions and line mentions per group. Run Mann-Whitney U on file mentions per group.
- QUAL-04: Assemble summary_table dict with all metrics, tests, p-values, and direction notes.

**Important constraints from research (MUST follow):**
- Reuse exact `classify_speech_act` from `analyze_fig5.py` — do NOT modify or improve the classifier
- Store `plan_question_ratio` as `None` (JSON null) for infinite cases — do NOT use float('inf') or large numbers
- Run Mann-Whitney U on finite ratios only — exclude None values
- Report counter-intuitive findings accurately: conflict trajectories have HIGHER planning rates and ratios — do not imply causation
- Include note about small no-plan-first sample (n=11) in first-turn planning results
- Include note about line number sparsity (1/428 messages) in specificity results

**Output JSON structure:**
```json
{
  "trajectories": [...],  // per-trajectory metrics (100 items)
  "plan_question_ratio": {  // QUAL-01 group comparison
    "no_conflict": {"mean": ..., "median": ..., "n_finite": ..., "n_infinite": ...},
    "conflict": {"mean": ..., "median": ..., "n_finite": ..., "n_infinite": ...},
    "test": "Mann-Whitney U", "statistic": ..., "p_value": ..., "direction": ...
  },
  "first_turn_planning": {  // QUAL-02 contingency table + Fisher's exact
    "contingency_table": {...}, "plan_first_conflict_rate": ...,
    "no_plan_first_conflict_rate": ..., "fishers_exact_odds_ratio": ...,
    "fishers_exact_p_value": ..., "n_plan_first": ..., "n_no_plan_first": ..., "note": ...
  },
  "specificity": {  // QUAL-03 group comparison
    "file_mentions": {"no_conflict": {"mean": ..., "median": ...}, "conflict": {...}, ...},
    "line_mentions": {"no_conflict": {...}, "conflict": {...}, "note": ...}
  },
  "summary_table": {  // QUAL-04 formatted comparison
    "plan_question_ratio": {...}, "first_turn_planning": {...},
    "file_mentions_per_trajectory": {...}, "line_mentions_per_trajectory": {...}
  },
  "metadata": {"input_file": ..., "n_coop_comm": 100, "n_clean": 59, "n_failed": 41, ...}
}
```

**Console output (`print_summary_table`):**
- Print formatted table with section headers for each QUAL metric
- Show group means/medians, rates, p-values, and direction notes
- Include sparsity and sample size caveats inline
  </action>
  <verify>
Verify the script exists and has correct structure:
```bash
python3 -c "
import ast, sys
tree = ast.parse(open('scripts/analyze_qualitative.py').read())
funcs = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
required = ['classify_speech_act', 'compute_trajectory_metrics', 'print_summary_table', 'main']
# classify_speech_act is imported not defined, so check others
for f in ['compute_trajectory_metrics', 'print_summary_table', 'main']:
    assert f in funcs, f'Missing function: {f}'
print('Structure OK')

# Verify import of classify_speech_act
source = open('scripts/analyze_qualitative.py').read()
assert 'from analyze_fig5 import classify_speech_act' in source, 'Missing classifier import'
assert 'fisher_exact' in source, 'Missing Fisher exact import'
assert 'mannwhitneyu' in source, 'Missing Mann-Whitney U import'
print('Imports OK')
"
```
  </verify>
  <done>
`scripts/analyze_qualitative.py` exists with: (1) imported `classify_speech_act` from `analyze_fig5`, (2) scipy statistical tests, (3) specificity regex patterns, (4) per-trajectory metric computation, (5) group comparison logic, (6) summary table console output, (7) JSON output structure covering all four QUAL requirements.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run analysis and validate output metrics</name>
  <files>data/qualitative_metrics.json</files>
  <action>
Run the qualitative analysis script and validate the output:

```bash
cd /mnt/data/terry/home/cooperbench-repro
python3 scripts/analyze_qualitative.py --input data/results.json --output data/qualitative_metrics.json
```

**Validation checks (run after script completes):**

1. Output file exists and is valid JSON
2. `trajectories` array has exactly 100 items (one per coop-comm record)
3. Each trajectory has fields: task_id, repo, merge_outcome, plan_count, question_count, plan_question_ratio, first_turn_is_plan, file_mentions, line_mentions
4. `plan_question_ratio` section has no_conflict and conflict sub-objects with mean, median, n_finite, n_infinite, and p_value
5. `first_turn_planning` section has contingency_table, Fisher's exact results, and note about small sample
6. `specificity` section has file_mentions and line_mentions with group comparisons
7. `summary_table` section has all four metric rows
8. No NaN or Infinity values in the JSON (check with `grep -c 'NaN\|Infinity'`)
9. Sum of n_finite + n_infinite per group equals group size (59 clean, 41 failed)

**Cross-check with known values from research:**
- Total messages should be 428 (consistent with fig5_metrics.json)
- ~89/100 trajectories should have first_turn_is_plan=true
- ~374/428 messages should have file mentions
- ~1/428 messages should have line number mentions
- n_no_plan_first should be ~11

If any validation fails, fix the script and re-run.
  </action>
  <verify>
```bash
cd /mnt/data/terry/home/cooperbench-repro
python3 -c "
import json
with open('data/qualitative_metrics.json') as f:
    m = json.load(f)

# Basic structure
assert len(m['trajectories']) == 100, f'Expected 100 trajectories, got {len(m[\"trajectories\"])}'
assert 'summary_table' in m, 'Missing summary_table'
assert 'plan_question_ratio' in m, 'Missing plan_question_ratio'
assert 'first_turn_planning' in m, 'Missing first_turn_planning'
assert 'specificity' in m, 'Missing specificity'

# No NaN/Infinity
s = json.dumps(m)
assert 'NaN' not in s, 'Contains NaN values'
assert 'Infinity' not in s, 'Contains Infinity values'

# Group sizes
pq = m['plan_question_ratio']
clean_total = pq['no_conflict']['n_finite'] + pq['no_conflict']['n_infinite']
failed_total = pq['conflict']['n_finite'] + pq['conflict']['n_infinite']
assert clean_total == 59, f'Clean group: expected 59, got {clean_total}'
assert failed_total == 41, f'Failed group: expected 41, got {failed_total}'

# First-turn planning
ft = m['first_turn_planning']
assert ft['n_plan_first'] + ft['n_no_plan_first'] == 100, 'Plan-first counts dont sum to 100'
assert 'note' in ft, 'Missing small sample note in first_turn_planning'

# p-values exist
assert 'p_value' in pq, 'Missing p_value in plan_question_ratio'
assert 'fishers_exact_p_value' in ft, 'Missing p_value in first_turn_planning'

print('All validations passed')
"
```
  </verify>
  <done>
`data/qualitative_metrics.json` exists with: (1) 100 per-trajectory metric records, (2) Plan:Question ratio group comparison with Mann-Whitney U p-value, (3) first-turn planning contingency table with Fisher's exact p-value and small-sample note, (4) specificity metrics with file/line mention counts and sparsity note, (5) summary comparison table for all four QUAL metrics, (6) no NaN or Infinity values, (7) group sizes matching 59 clean + 41 failed.
  </done>
</task>

</tasks>

<verification>
1. `python3 scripts/analyze_qualitative.py` runs without errors and produces console output
2. `data/qualitative_metrics.json` contains valid JSON with all required sections
3. Plan:Question ratios computed with None for infinite cases, Mann-Whitney U test on finite values only
4. First-turn planning correlated via Fisher's exact test with p-value reported
5. File/line mention counts computed per trajectory with appropriate notes about sparsity
6. Summary table compares all metrics for conflict vs no-conflict groups
7. Speech act classification consistent with Phase 3 (uses imported classify_speech_act)
</verification>

<success_criteria>
- `scripts/analyze_qualitative.py` exists, is runnable, and imports classify_speech_act from analyze_fig5
- `data/qualitative_metrics.json` contains per-trajectory metrics (100 records), group comparisons with p-values, and summary table
- QUAL-01: Plan:Question ratio computed per trajectory, Mann-Whitney U p-value reported for correlation with merge outcome
- QUAL-02: First-turn planning detected, Fisher's exact test on 2x2 table, small sample note included
- QUAL-03: File mentions and line mentions counted per trajectory, line sparsity noted
- QUAL-04: Summary table comparing all metrics for conflict vs no-conflict groups with statistical tests
</success_criteria>

<output>
After completion, create `.planning/phases/05-qualitative-transcript-analysis/05-01-SUMMARY.md`
</output>
