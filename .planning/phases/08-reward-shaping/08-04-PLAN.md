---
phase: 08-reward-shaping
plan: 04
type: execute
wave: 3
depends_on: ["08-02", "08-03"]
files_modified:
  - src/training/wandb_enrichment/debate_streamer.py
  - src/training/wandb_enrichment/metric_schema.py
  - configs/sweep_math_debate_grpo.py
  - tests/test_reward_shaping_integration.py
autonomous: true

must_haves:
  truths:
    - "DebateMetricStreamer applies configured reward shaping strategy to batch rewards before computing metrics"
    - "SWEEP config has reward_shaping_strategy and reward_shaping_params fields on DebateMetricStreamerConfig"
    - "Default behavior (no config) uses identity strategy, preserving existing binary SymPy reward unchanged"
    - "Shaped rewards are logged to W&B alongside raw rewards for comparison"
    - "New reward shaping strategies can be selected by setting strategy name in SWEEP config"
  artifacts:
    - path: "src/training/wandb_enrichment/debate_streamer.py"
      provides: "DebateMetricStreamer with reward shaping integration"
      contains: "create_strategy_from_config"
    - path: "src/training/wandb_enrichment/metric_schema.py"
      provides: "Shaped reward metric constants"
      contains: "METRIC_SHAPED_REWARD"
    - path: "configs/sweep_math_debate_grpo.py"
      provides: "SWEEP config with reward shaping fields"
      contains: "reward_shaping_strategy"
    - path: "tests/test_reward_shaping_integration.py"
      provides: "Integration test for reward shaping in streamer pipeline"
      min_lines: 80
  key_links:
    - from: "src/training/wandb_enrichment/debate_streamer.py"
      to: "src/training/reward_shaping/registry.py"
      via: "create_strategy_from_config in __init__"
      pattern: "create_strategy_from_config"
    - from: "configs/sweep_math_debate_grpo.py"
      to: "src/training/wandb_enrichment/debate_streamer.py"
      via: "DebateMetricStreamerConfig fields"
      pattern: "reward_shaping_strategy"
---

<objective>
Wire reward shaping into the training pipeline: integrate with DebateMetricStreamer, add SWEEP config surface, add shaped reward metrics, and verify end-to-end with integration tests.

Purpose: This plan connects the reward shaping strategies (Plans 01-03) to the live training pipeline so they actually shape rewards during training. Without this wiring, the strategies exist but are never invoked.

Output: Updated DebateMetricStreamer, SWEEP config fields, shaped reward metrics, and integration tests.
</objective>

<execution_context>
@/home/terry_tong_cohere_com/.claude/get-shit-done/workflows/execute-plan.md
@/home/terry_tong_cohere_com/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-reward-shaping/08-01-SUMMARY.md
@.planning/phases/08-reward-shaping/08-02-SUMMARY.md
@.planning/phases/08-reward-shaping/08-03-SUMMARY.md

# Files being modified
@multiagent/src/training/wandb_enrichment/debate_streamer.py
@multiagent/src/training/wandb_enrichment/metric_schema.py
@multiagent/configs/sweep_math_debate_grpo.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add shaped reward metrics and update DebateMetricStreamerConfig</name>
  <files>
    src/training/wandb_enrichment/metric_schema.py
    src/training/wandb_enrichment/debate_streamer.py
  </files>
  <action>
    **metric_schema.py -- add shaped reward metric constants:**

    Add a new section "Shaped Reward Metrics" after the existing Per-Role Reward section:

    ```python
    # ============================================================================
    # Shaped Reward Metrics
    # ============================================================================

    METRIC_SHAPED_REWARD_MEAN = f"{DEBATE_PREFIX}shaped_reward/mean"
    """Mean shaped reward across batch (after reward shaping strategy applied)."""

    METRIC_SHAPED_REWARD_SOLVER = f"{DEBATE_PREFIX}shaped_reward/solver"
    """Shaped reward for solver role (when strategy returns per-role rewards)."""

    METRIC_SHAPED_REWARD_VERIFIER = f"{DEBATE_PREFIX}shaped_reward/verifier"
    """Shaped reward for verifier role (when strategy returns per-role rewards)."""

    METRIC_SHAPED_REWARD_JUDGE = f"{DEBATE_PREFIX}shaped_reward/judge"
    """Shaped reward for judge role (when strategy returns per-role rewards)."""

    METRIC_REWARD_SHAPING_STRATEGY = f"{DEBATE_PREFIX}reward_shaping/strategy_name"
    """Name of active reward shaping strategy (logged as text metric on first step)."""
    ```

    Add these to ALL_DEBATE_METRICS list.

    **debate_streamer.py -- add reward shaping config fields and integration:**

    1. Add `reward_shaping_strategy: str = ""` field to DebateMetricStreamerConfig (both Flink and stub versions). Empty string means identity/passthrough (backward compatible).
    2. Add `reward_shaping_params: dict = {}` field to DebateMetricStreamerConfig. Passed as **kwargs to strategy constructor.
    3. In `_DebateMetricStreamerImpl.__init__()`:
       - Import create_strategy_from_config from src.training.reward_shaping
       - Build config dict: `{"strategy_name": config.reward_shaping_strategy, "strategy_params": config.reward_shaping_params}` if reward_shaping_strategy is non-empty, else None
       - `self._reward_shaper = create_strategy_from_config(shaper_config)`
       - Log strategy name: `logger.info(f"DebateMetricStreamer: using reward shaping strategy: {self._reward_shaper.name}")`
    4. In `_DebateMetricStreamerImpl.get()`, AFTER extracting rewards and role_labels but BEFORE computing debate metrics:
       - Build role_masks from items if available (extract from item.data if present, else None)
       - Build trajectory_metadata from items (extract relevant metadata dicts)
       - `shaped_result = self._reward_shaper.shape_rewards(rewards, role_masks=role_masks, trajectory_metadata=trajectory_metadata)`
       - If shaped_result is dict (per-role rewards):
         - Log METRIC_SHAPED_REWARD_SOLVER, METRIC_SHAPED_REWARD_VERIFIER, METRIC_SHAPED_REWARD_JUDGE
         - Compute mean for METRIC_SHAPED_REWARD_MEAN
       - If shaped_result is np.ndarray:
         - Log METRIC_SHAPED_REWARD_MEAN = shaped_result.mean()
       - Add shaped_reward metrics to debate_metrics dict

    **Important backward compatibility:**
    - When reward_shaping_strategy is "" (default), identity strategy is used, and shaped_reward metrics equal raw reward metrics. This means existing SWEEP configs work unchanged.
    - The raw reward metrics (METRIC_REWARD_SOLVER etc.) continue to be computed from ORIGINAL rewards, NOT shaped rewards. This allows W&B dashboards to compare raw vs shaped side by side.
    - Error handling: if reward shaping fails, log warning and fall through to raw rewards (matching existing error handling pattern in get()).
  </action>
  <verify>
    `cd /home/terry_tong_cohere_com/multiagent && uv run python -c "from src.training.wandb_enrichment.metric_schema import METRIC_SHAPED_REWARD_MEAN; print(METRIC_SHAPED_REWARD_MEAN)"` -- prints "debate/shaped_reward/mean"
    `cd /home/terry_tong_cohere_com/multiagent && uv run python -c "from src.training.wandb_enrichment.debate_streamer import DebateMetricStreamerConfig; c = DebateMetricStreamerConfig(); print(c.reward_shaping_strategy)"` -- prints ""
  </verify>
  <done>
    DebateMetricStreamerConfig has reward_shaping_strategy and reward_shaping_params fields. Metric schema includes shaped reward constants. Streamer initializes reward shaper from config and applies it in get().
  </done>
</task>

<task type="auto">
  <name>Task 2: Update SWEEP config and create integration tests</name>
  <files>
    configs/sweep_math_debate_grpo.py
    tests/test_reward_shaping_integration.py
  </files>
  <action>
    **configs/sweep_math_debate_grpo.py:**

    Add commented-out reward shaping fields to the DebateMetricStreamerConfig instantiation in actor_outputs_streamers, showing users how to enable different strategies:

    ```python
    # Phase 8: Debate metric enrichment with reward shaping
    # Computes per-role rewards, zero-advantage detection, and shaped rewards
    DebateMetricStreamerConfig(
        n_rollouts_per_prompt=GENERATIONS_PER_PROMPT,
        # Reward shaping strategy (default: identity/passthrough)
        # Options: "identity", "difference_rewards", "coma_advantage",
        #          "potential_based", "reward_mixing"
        # reward_shaping_strategy="difference_rewards",
        # reward_shaping_params={},  # Strategy-specific params, e.g. {"alpha": 0.7} for reward_mixing
    ),
    ```

    This keeps the default behavior unchanged while documenting the new fields for users.

    **tests/test_reward_shaping_integration.py:**

    Create integration tests that verify the full pipeline from config to shaped rewards:

    1. `test_streamer_with_identity_strategy` -- Create DebateMetricStreamerConfig with no reward_shaping_strategy. Mock upstream items with known rewards. Verify get() returns metrics with shaped_reward/mean equal to raw reward mean.

    2. `test_streamer_with_difference_rewards_strategy` -- Create config with reward_shaping_strategy="difference_rewards". Mock items with rewards and counterfactual metadata. Verify shaped_reward metrics are populated with per-role values.

    3. `test_streamer_with_reward_mixing_strategy` -- Create config with reward_shaping_strategy="reward_mixing", reward_shaping_params={"alpha": 0.5}. Verify shaped rewards blend correctly.

    4. `test_streamer_with_potential_based_strategy` -- Create config with reward_shaping_strategy="potential_based", reward_shaping_params={"gamma": 0.99, "potential_type": "debate_length"}. Verify shaped rewards include potential shaping term.

    5. `test_streamer_with_coma_advantage_strategy` -- Create config with reward_shaping_strategy="coma_advantage", reward_shaping_params={"n_rollouts_per_prompt": 4}. Verify advantage computation.

    6. `test_streamer_backward_compatible_no_config` -- Create DebateMetricStreamerConfig() with all defaults. Verify behavior identical to pre-Phase-8 (raw rewards pass through, all existing metrics unchanged).

    7. `test_all_strategies_in_registry` -- Verify all 5 strategies are registered: identity, difference_rewards, coma_advantage, potential_based, reward_mixing.

    Use a mock upstream streamer pattern (same as existing debate_streamer tests): create a SimpleUpstream class that returns canned items and metrics from get().

    For items, create mock objects with:
    - `item.data = {"rewards": np.array(reward_value)}`
    - `item.metadata = {"env_name": np.array("math_debate"), "math_debate/reward_metrics": {"role_label": "solver"}}`

    **Test structure note:** These are integration tests, not unit tests. They verify that DebateMetricStreamer correctly instantiates and invokes the reward shaping strategy, not the strategy logic itself (which is tested in Plans 02 and 03).
  </action>
  <verify>
    `cd /home/terry_tong_cohere_com/multiagent && uv run pytest tests/test_reward_shaping_integration.py -v`
    All 7 integration tests pass.

    `cd /home/terry_tong_cohere_com/multiagent && uv run pytest tests/test_reward_shaping_registry.py tests/test_difference_rewards.py tests/test_reward_mixing.py tests/test_coma_advantage.py tests/test_potential_shaping.py tests/test_reward_shaping_integration.py -v`
    ALL reward shaping tests pass (full suite).
  </verify>
  <done>
    SWEEP config documents reward shaping fields with commented examples. Integration tests verify end-to-end wiring from config through DebateMetricStreamer to shaped reward metrics. Backward compatibility confirmed: default config behavior unchanged.
  </done>
</task>

</tasks>

<verification>
- `uv run pytest tests/test_reward_shaping_integration.py -v` -- all 7 integration tests pass
- `uv run pytest tests/ -k "reward_shaping" -v` -- all reward shaping tests pass (unit + integration)
- `uv run ruff check src/training/reward_shaping/ configs/sweep_math_debate_grpo.py` -- no lint errors
- Verify SWEEP config still parses: `uv run python -c "import configs.sweep_math_debate_grpo"` -- no import errors
- Verify backward compatibility: DebateMetricStreamerConfig() with defaults produces identity strategy behavior
</verification>

<success_criteria>
- DebateMetricStreamer applies reward shaping strategy from config during get()
- Raw reward metrics (debate/reward/*) always use original rewards (for comparison)
- Shaped reward metrics (debate/shaped_reward/*) use shaped rewards
- Default config (empty strategy) produces identity behavior -- fully backward compatible
- SWEEP config documents all available strategies
- All 5 strategies registered and retrievable
- Full test suite passes (unit tests from Plans 01-03 + integration tests from this plan)
</success_criteria>

<output>
After completion, create `.planning/phases/08-reward-shaping/08-04-SUMMARY.md`
</output>
