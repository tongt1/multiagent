---
phase: 02-results-collection-and-data-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - repos/CooperBench/src/cooperbench/eval/backends/docker.py
autonomous: true
requirements: [DATA-02, DATA-03]

must_haves:
  truths:
    - "Docker eval backend creates containers that stay running (entrypoint override works)"
    - "Eval produces eval.json for all 100 solo runs with both_passed and feature-level results"
    - "Eval produces eval.json for all 100 coop-comm runs with merge status/strategy and feature-level results"
    - "Eval produces eval.json for all 100 coop-nocomm runs with merge status/strategy and feature-level results"
    - "Eval errors are captured in eval.json error field, not silently lost"
  artifacts:
    - path: "repos/CooperBench/src/cooperbench/eval/backends/docker.py"
      provides: "Fixed Docker eval backend with entrypoint override"
      contains: 'entrypoint=""'
    - path: "repos/CooperBench/logs/command-a-solo/eval_summary.json"
      provides: "Solo eval summary with pass/fail/error counts"
    - path: "repos/CooperBench/logs/command-a-coop-comm/eval_summary.json"
      provides: "Coop-comm eval summary with pass/fail/error counts"
    - path: "repos/CooperBench/logs/command-a-coop-nocomm/eval_summary.json"
      provides: "Coop-nocomm eval summary with pass/fail/error counts"
  key_links:
    - from: "cooperbench eval CLI"
      to: "DockerBackend.create_sandbox()"
      via: "entrypoint='' parameter"
      pattern: 'entrypoint=""'
    - from: "eval.json"
      to: "result.json"
      via: "same log_dir directory"
      pattern: "eval\\.json"
---

<objective>
Fix the Docker eval backend entrypoint bug and run evaluation on all 300 existing benchmark results (3 settings x 100 pairs).

Purpose: Eval produces per-run eval.json files containing test pass/fail results and merge outcomes, which are the primary data source for the unified results store. Without eval, we only have agent run metadata (patches, cost, steps) but no actual test results.

Output: 300 eval.json files across 3 run directories, plus eval_summary.json per setting.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-results-collection-and-data-foundation/02-RESEARCH.md
@repos/CooperBench/src/cooperbench/eval/backends/docker.py
@repos/CooperBench/src/cooperbench/eval/evaluate.py
@repos/CooperBench/src/cooperbench/eval/runs.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix Docker eval backend entrypoint and smoke-test eval</name>
  <files>repos/CooperBench/src/cooperbench/eval/backends/docker.py</files>
  <action>
1. Edit `repos/CooperBench/src/cooperbench/eval/backends/docker.py`, in `DockerBackend.create_sandbox()` method (line 93). Add `entrypoint=""` parameter to the `client.containers.run()` call, between `command="sleep infinity"` and `detach=True`. This overrides the CooperBench Docker image ENTRYPOINT (`/usr/local/bin/runner.sh`) that causes containers to exit immediately.

   The fixed call should look like:
   ```python
   container = client.containers.run(
       image=image,
       command="sleep infinity",
       entrypoint="",          # Override CooperBench image ENTRYPOINT=/usr/local/bin/runner.sh
       detach=True,
       working_dir=workdir,
       remove=False,
       stop_signal="SIGTERM",
   )
   ```

   This matches the pattern already used in `agents/mini_swe_agent/environments/docker.py` line 59.

2. Clean up any leftover cooperbench containers from previous runs:
   ```bash
   docker ps -a --filter "ancestor=akhatua/cooperbench" -q | xargs -r docker rm -f
   ```

3. Smoke-test eval on ONE task from the solo run to confirm the fix works:
   ```bash
   cd /mnt/data/terry/home/cooperbench-repro/repos/CooperBench
   source .venv/bin/activate
   cooperbench eval -n command-a-solo --backend docker --repo dspy_task --task 8394 -f 3,4
   ```

4. Verify the smoke test produced an eval.json:
   ```bash
   cat logs/command-a-solo/solo/dspy_task/8394/f3_f4/eval.json
   ```
   Confirm it contains: repo, task_id, features, setting, merge (null for solo), feature1, feature2, both_passed, error, evaluated_at.

5. Smoke-test ONE coop-comm task to confirm merge logic works:
   ```bash
   cooperbench eval -n command-a-coop-comm --backend docker --repo dspy_task --task 8394 -f 3,4
   cat logs/command-a-coop-comm/coop/dspy_task/8394/f3_f4/eval.json
   ```
   Confirm eval.json has merge field with status and strategy.
  </action>
  <verify>
  - `grep 'entrypoint=""' repos/CooperBench/src/cooperbench/eval/backends/docker.py` returns a match
  - `cat repos/CooperBench/logs/command-a-solo/solo/dspy_task/8394/f3_f4/eval.json` shows valid JSON with both_passed field
  - `cat repos/CooperBench/logs/command-a-coop-comm/coop/dspy_task/8394/f3_f4/eval.json` shows valid JSON with merge field containing status and strategy
  </verify>
  <done>Docker eval backend entrypoint fix confirmed working. Solo eval produces eval.json with test results. Coop eval produces eval.json with merge outcomes and test results.</done>
</task>

<task type="auto">
  <name>Task 2: Run full evaluation on all 300 existing benchmark results</name>
  <files></files>
  <action>
IMPORTANT: All eval commands MUST be run from `repos/CooperBench/` directory (eval resolves `logs/` and `dataset/` relative to cwd). The smoke-test eval.json files from Task 1 will be skipped automatically (already evaluated, no --force needed).

1. Clean up any leftover Docker containers:
   ```bash
   docker ps -a --filter "ancestor=akhatua/cooperbench" -q | xargs -r docker rm -f
   ```

2. Run eval for all 3 settings. Per user decision, run all 3 in parallel (30 concurrent Docker containers: 10 per setting). Run these as 3 parallel background processes:
   ```bash
   cd /mnt/data/terry/home/cooperbench-repro/repos/CooperBench
   source .venv/bin/activate

   cooperbench eval -n command-a-solo --backend docker -c 10 &
   cooperbench eval -n command-a-coop-comm --backend docker -c 10 &
   cooperbench eval -n command-a-coop-nocomm --backend docker -c 10 &
   wait
   ```

   If any eval fails, retry once per user decision:
   ```bash
   cooperbench eval -n <failed-run-name> --backend docker -c 10
   ```
   Runs that already have eval.json will be auto-skipped (no --force).

3. After all evals complete, count eval.json files per setting:
   ```bash
   find logs/command-a-solo -name "eval.json" | wc -l          # expect 100
   find logs/command-a-coop-comm -name "eval.json" | wc -l     # expect 100
   find logs/command-a-coop-nocomm -name "eval.json" | wc -l   # expect 100
   ```

4. Report eval errors: check eval_summary.json for each setting:
   ```bash
   python3 -c "
   import json
   for name in ['command-a-solo', 'command-a-coop-comm', 'command-a-coop-nocomm']:
       with open(f'logs/{name}/eval_summary.json') as f:
           s = json.load(f)
       print(f'{name}: {s[\"passed\"]} pass, {s[\"failed\"]} fail, {s[\"errors\"]} errors')
   "
   ```

5. Clean up Docker containers after eval:
   ```bash
   docker ps -a --filter "ancestor=akhatua/cooperbench" -q | xargs -r docker rm -f
   ```

NOTE: This task is LONG-RUNNING. Expect ~2-4 hours for all 300 evals. Each eval spins up a Docker container, applies patches, runs tests (up to 300s timeout per test), then tears down. With 30 concurrent containers across 3 parallel eval processes, wall-clock time should be ~1-2 hours if tests are fast, up to 4 hours if many hit the 300s timeout.
  </action>
  <verify>
  - `find repos/CooperBench/logs/command-a-solo -name "eval.json" | wc -l` returns 100
  - `find repos/CooperBench/logs/command-a-coop-comm -name "eval.json" | wc -l` returns 100
  - `find repos/CooperBench/logs/command-a-coop-nocomm -name "eval.json" | wc -l` returns 100
  - Total eval.json count: 300
  - Each eval_summary.json exists and has reasonable pass/fail/error breakdown
  </verify>
  <done>All 300 benchmark runs evaluated. eval.json files exist in all 300 run directories containing test pass/fail results and merge outcomes. Eval errors (if any) are recorded in eval.json error field and counted separately in eval_summary.json.</done>
</task>

</tasks>

<verification>
1. `grep 'entrypoint=""' repos/CooperBench/src/cooperbench/eval/backends/docker.py` -- fix applied
2. `find repos/CooperBench/logs -name "eval.json" | wc -l` -- returns 300
3. Sample eval.json validation:
   ```bash
   python3 -c "
   import json; from pathlib import Path
   for setting in ['command-a-solo', 'command-a-coop-comm', 'command-a-coop-nocomm']:
       evals = list(Path(f'repos/CooperBench/logs/{setting}').rglob('eval.json'))
       valid = sum(1 for e in evals if 'both_passed' in json.load(open(e)))
       print(f'{setting}: {len(evals)} total, {valid} valid')
   "
   ```
4. Coop eval.json files have merge field with status and strategy:
   ```bash
   python3 -c "
   import json; from pathlib import Path
   evals = list(Path('repos/CooperBench/logs/command-a-coop-comm').rglob('eval.json'))
   with_merge = sum(1 for e in evals if json.load(open(e)).get('merge') is not None)
   print(f'Coop-comm evals with merge data: {with_merge}/{len(evals)}')
   "
   ```
</verification>

<success_criteria>
- 300 eval.json files exist (100 per setting)
- Docker eval backend has entrypoint fix applied
- Solo eval.json files have merge=null, feature1/feature2 results, both_passed
- Coop eval.json files have merge.status, merge.strategy, feature1/feature2 results, both_passed
- Eval errors captured and counted separately
</success_criteria>

<output>
After completion, create `.planning/phases/02-results-collection-and-data-foundation/02-01-SUMMARY.md`
</output>
