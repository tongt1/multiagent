---
phase: 02-results-collection-and-data-foundation
plan: 03
type: execute
wave: 3
depends_on: ["02-01", "02-02"]
files_modified:
  - scripts/collect_results.py
  - data/results.json
autonomous: true
requirements: [DATA-01, DATA-02, DATA-03, FIG4-01, FIG4-02]

must_haves:
  truths:
    - "A unified JSON results store exists at data/results.json containing all benchmark outcomes"
    - "Each record includes task ID, setting, merge outcome, test outcome, and status"
    - "Merge outcomes are tracked as independent dimension: merge_clean/merge_union/merge_failed x tests_pass/tests_fail"
    - "Infrastructure errors are excluded from metric computations and their count is reported separately"
    - "Per-pair difficulty scores d(pair) = 1 - mean(solo_both_passed) are computed from 3 seeds"
    - "Each record has a bucket assignment (0-9) from 10 equal-width buckets over [0,1]"
    - "LimitsExceeded pairs are marked as both_passed=false regardless of eval result"
    - "Solo records have merge_outcome=merge_clean (implicit)"
    - "Coop-comm records include conversation messages array"
    - "Difficulty and bucket values propagate to ALL records (not just solo)"
  artifacts:
    - path: "scripts/collect_results.py"
      provides: "Collection pipeline script"
      min_lines: 100
    - path: "data/results.json"
      provides: "Unified results store with all benchmark outcomes"
  key_links:
    - from: "scripts/collect_results.py"
      to: "repos/CooperBench/logs/*/result.json"
      via: "pathlib glob traversal"
      pattern: "result\\.json"
    - from: "scripts/collect_results.py"
      to: "repos/CooperBench/logs/*/eval.json"
      via: "pathlib glob traversal"
      pattern: "eval\\.json"
    - from: "scripts/collect_results.py"
      to: "data/results.json"
      via: "json.dump output"
      pattern: "results\\.json"
    - from: "data/results.json"
      to: "Phase 3 analysis modules"
      via: "downstream consumer reads results.json"
      pattern: "data/results\\.json"
---

<objective>
Build the collection script that reads all result.json, eval.json, and conversation.json files from all run directories, normalizes them into a flat array of unified records, computes per-pair difficulty scores from 3 solo seeds, assigns difficulty buckets, and writes the final data store to data/results.json.

Purpose: This is the single data artifact that all downstream analysis (Phases 3-5) consumes. Every figure, metric, and qualitative analysis reads from data/results.json. The collection script codifies all user decisions about merge outcome mapping, LimitsExceeded handling, difficulty computation, and record schema.

Output: `scripts/collect_results.py` (rerunnable pipeline) and `data/results.json` (unified data store).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-results-collection-and-data-foundation/02-RESEARCH.md
@.planning/phases/02-results-collection-and-data-foundation/02-01-SUMMARY.md
@.planning/phases/02-results-collection-and-data-foundation/02-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build collection script and produce unified results store</name>
  <files>scripts/collect_results.py, data/results.json</files>
  <action>
Create `scripts/collect_results.py` -- a pure-Python script (stdlib only: json, pathlib, collections, statistics, argparse) that:

**1. Run name configuration:**
```python
RUN_SETTINGS = {
    "command-a-solo": "solo",
    "command-a-solo-seed1": "solo",
    "command-a-solo-seed2": "solo",
    "command-a-coop-comm": "coop-comm",
    "command-a-coop-nocomm": "coop-nocomm",
}
RUN_SEEDS = {
    "command-a-solo": 0,
    "command-a-solo-seed1": 1,
    "command-a-solo-seed2": 2,
}
```

**2. Per-run collection logic (`collect_run`):**
For each run_name, traverse `repos/CooperBench/logs/{run_name}/{solo|coop}/{repo}/{task_id}/f{X}_f{Y}/` directories. For each directory:
- Read `result.json` for run metadata
- Read `eval.json` for test results (REQUIRED -- skip directory if missing, log warning)
- Read `conversation.json` for coop-comm messages (optional, empty array if missing)

**3. Record normalization:**
Each record in the unified store follows this schema (Claude's Discretion on exact field names, using the recommended schema from RESEARCH.md):
```python
{
    "repo": str,                    # from result.json
    "task_id": int,                 # from result.json
    "features": [int, int],         # from result.json
    "setting": str,                 # mapped from run_name (solo/coop-comm/coop-nocomm)
    "seed": int,                    # 0/1/2 for solo seeds, 0 for coop
    "run_name": str,                # original run name
    "model": str,                   # from result.json (command-a-03-2025)
    "started_at": str,              # from result.json
    "duration_seconds": float,      # from result.json
    "total_cost": float,            # from result.json
    "total_steps": int,             # from result.json
    "agent_status": str,            # flattened (see below)
    "infra_error": bool,            # from result.json infra_error field (default false)
    "both_passed": bool,            # from eval.json, OVERRIDDEN for LimitsExceeded
    "feature1_passed": bool,        # from eval.json feature1.passed
    "feature2_passed": bool,        # from eval.json feature2.passed
    "merge_outcome": str,           # derived (see below)
    "merge_status": str|None,       # from eval.json merge.status (null for solo)
    "merge_strategy": str|None,     # from eval.json merge.strategy (null for solo)
    "eval_error": str|None,         # from eval.json error field
    "messages": list,               # from conversation.json (empty for solo/nocomm)
    "messages_count": int,          # len(messages)
    "difficulty": float|None,       # computed later, null initially
    "bucket": int|None,             # computed later, null initially
}
```

**4. Agent status flattening:**
- Solo: `result["agent"]["status"]` directly
- Coop: worst status across agents. Priority: Error > LimitsExceeded > Submitted > Unknown
  ```python
  def get_agent_status(result_data, setting):
      if setting == "solo":
          return result_data.get("agent", {}).get("status", "Unknown")
      agents = result_data.get("agents", {})
      statuses = [a.get("status", "Unknown") for a in agents.values()]
      priority = {"Error": 0, "LimitsExceeded": 1, "Submitted": 2, "Unknown": 3}
      return min(statuses, key=lambda s: priority.get(s, 99))
  ```

**5. LimitsExceeded override (LOCKED DECISION):**
Per user decision: LimitsExceeded pairs count as failure. After reading eval.json `both_passed`, override:
```python
if agent_status == "LimitsExceeded":
    record["both_passed"] = False
```

**6. Merge outcome classification (LOCKED DECISION):**
```python
def classify_merge_outcome(eval_data, setting):
    if setting == "solo":
        return "merge_clean"  # per user decision
    merge = eval_data.get("merge")
    if merge is None:
        return "merge_failed"
    status = merge.get("status", "error")
    strategy = merge.get("strategy")
    if status == "clean" and strategy == "naive":
        return "merge_clean"
    elif status == "clean" and strategy == "union":
        return "merge_union"
    else:
        return "merge_failed"
```

**7. Difficulty score computation:**
After collecting ALL records:
```python
def compute_difficulty(records):
    pair_key = lambda r: (r["repo"], r["task_id"], tuple(r["features"]))
    solo = [r for r in records if r["setting"] == "solo"]

    pair_passes = defaultdict(list)
    for r in solo:
        pair_passes[pair_key(r)].append(r["both_passed"])

    pair_difficulty = {}
    for key, passes in pair_passes.items():
        d = 1.0 - sum(passes) / len(passes)
        bucket = min(int(d * 10), 9)
        pair_difficulty[key] = (round(d, 4), bucket)

    # Apply to ALL records (all settings, all seeds)
    for r in records:
        key = pair_key(r)
        if key in pair_difficulty:
            r["difficulty"], r["bucket"] = pair_difficulty[key]
```

**8. Output and reporting:**
- Write `data/results.json` with `json.dump(records, f, indent=2)`
- Print summary:
  - Total records by setting
  - Infra error count (should be 0 from Phase 1)
  - LimitsExceeded override count
  - Eval error count
  - Pass rates by setting (excluding infra errors)
  - Difficulty distribution (count per difficulty value)
  - Bucket population (count per bucket 0-9)

**9. CLI interface:**
```python
parser = argparse.ArgumentParser()
parser.add_argument("--skip-difficulty", action="store_true",
                    help="Collect without computing difficulty scores")
parser.add_argument("--cooperbench-dir", type=Path,
                    default=Path(__file__).resolve().parent.parent / "repos" / "CooperBench",
                    help="Path to CooperBench repo root")
parser.add_argument("--output", type=Path,
                    default=Path(__file__).resolve().parent.parent / "data" / "results.json",
                    help="Output file path")
```

**After creating the script:**
Run it:
```bash
cd /mnt/data/terry/home/cooperbench-repro
python scripts/collect_results.py
```
  </action>
  <verify>
  - `python scripts/collect_results.py` runs without errors
  - `python -c "import json; d=json.load(open('data/results.json')); print(len(d))"` returns expected total (500 for 3 solo seeds + coop-comm + coop-nocomm = 300 + 100 + 100)
  - All records have non-null difficulty and bucket fields
  - `python -c "import json; d=json.load(open('data/results.json')); print(set(r['setting'] for r in d))"` shows {'solo', 'coop-comm', 'coop-nocomm'}
  - `python -c "import json; d=json.load(open('data/results.json')); print(sorted(set(r['difficulty'] for r in d)))"` shows values from {0.0, 0.33, 0.67, 1.0} (approximately)
  </verify>
  <done>Collection script created and data/results.json produced with all benchmark records, difficulty scores, and bucket assignments.</done>
</task>

<task type="auto">
  <name>Task 2: Validate unified results store against phase requirements</name>
  <files></files>
  <action>
Run comprehensive validation checks on `data/results.json` to confirm all phase requirements are met:

```python
#!/usr/bin/env python3
"""Validate data/results.json against Phase 2 requirements."""
import json
from pathlib import Path
from collections import Counter, defaultdict

with open("data/results.json") as f:
    records = json.load(f)

print(f"Total records: {len(records)}")
print()

# DATA-01: Unified JSON store with all settings
settings = Counter(r["setting"] for r in records)
print("DATA-01: Records by setting:")
for s, c in sorted(settings.items()):
    print(f"  {s}: {c}")
assert "solo" in settings
assert "coop-comm" in settings
assert "coop-nocomm" in settings
print("  PASS: All 3 settings present")
print()

# DATA-02: Infrastructure errors distinguished
infra = [r for r in records if r["infra_error"]]
limits = [r for r in records if r["agent_status"] == "LimitsExceeded"]
eval_errors = [r for r in records if r["eval_error"] is not None]
print("DATA-02: Error breakdown:")
print(f"  infra_error=true: {len(infra)}")
print(f"  agent_status=LimitsExceeded: {len(limits)}")
print(f"  eval_error!=null: {len(eval_errors)}")
# Verify LimitsExceeded override
for r in limits:
    assert r["both_passed"] == False, f"LimitsExceeded pair {r['repo']}/{r['task_id']} has both_passed=True!"
print(f"  PASS: All {len(limits)} LimitsExceeded pairs have both_passed=False")
print()

# DATA-03: Merge outcomes as separate dimension from test outcomes
merge_outcomes = Counter(r["merge_outcome"] for r in records)
print("DATA-03: Merge outcome distribution:")
for m, c in sorted(merge_outcomes.items()):
    print(f"  {m}: {c}")
# Cross-product check
cross = Counter((r["merge_outcome"], r["both_passed"]) for r in records if r["setting"] != "solo")
print("  Merge x Test cross-product (coop only):")
for (m, t), c in sorted(cross.items()):
    print(f"    {m} x both_passed={t}: {c}")
print("  PASS: Merge and test tracked independently")
print()

# FIG4-01: Difficulty scores computed
difficulties = [r["difficulty"] for r in records if r["difficulty"] is not None]
print(f"FIG4-01: Records with difficulty: {len(difficulties)}/{len(records)}")
unique_diffs = sorted(set(round(d, 2) for d in difficulties))
print(f"  Unique difficulty values: {unique_diffs}")
assert len(difficulties) == len(records), "Some records missing difficulty!"
print("  PASS: All records have difficulty scores")
print()

# FIG4-02: Bucket assignments
buckets = Counter(r["bucket"] for r in records if r["bucket"] is not None)
print("FIG4-02: Bucket population (per-record counts, all settings):")
for b in range(10):
    count = buckets.get(b, 0)
    print(f"  bucket {b}: {count}")

# Also show per-pair bucket distribution (what matters for Figure 4)
pair_key = lambda r: (r["repo"], r["task_id"], tuple(r["features"]))
unique_pairs = {}
for r in records:
    k = pair_key(r)
    if k not in unique_pairs:
        unique_pairs[k] = r["bucket"]
pair_buckets = Counter(unique_pairs.values())
print("  Per-pair bucket distribution (100 unique pairs):")
for b in range(10):
    count = pair_buckets.get(b, 0)
    print(f"    bucket {b}: {count} pairs")
populated = sum(1 for b in range(10) if pair_buckets.get(b, 0) > 0)
print(f"  Populated buckets: {populated}/10")
print("  PASS: Bucket assignments computed")
print()

# Schema completeness check
required_fields = [
    "repo", "task_id", "features", "setting", "seed", "run_name", "model",
    "started_at", "duration_seconds", "total_cost", "total_steps",
    "agent_status", "infra_error", "both_passed", "feature1_passed",
    "feature2_passed", "merge_outcome", "merge_status", "merge_strategy",
    "eval_error", "messages", "messages_count", "difficulty", "bucket"
]
sample = records[0]
missing = [f for f in required_fields if f not in sample]
print(f"Schema check: {len(required_fields)} required fields")
if missing:
    print(f"  MISSING: {missing}")
else:
    print("  PASS: All fields present")
print()

# Coop-comm messages check
comm_records = [r for r in records if r["setting"] == "coop-comm"]
with_messages = sum(1 for r in comm_records if r["messages_count"] > 0)
print(f"Messages: {with_messages}/{len(comm_records)} coop-comm records have messages")
total_msgs = sum(r["messages_count"] for r in comm_records)
print(f"  Total messages embedded: {total_msgs}")
print()

# Pass rates by setting (excluding infra errors, for context only)
print("Pass rates by setting (excluding infra errors):")
for setting in ["solo", "coop-comm", "coop-nocomm"]:
    setting_records = [r for r in records if r["setting"] == setting and not r["infra_error"]]
    # For solo, only count seed 0 (original run) to avoid inflating denominator
    if setting == "solo":
        setting_records = [r for r in setting_records if r["seed"] == 0]
    passed = sum(1 for r in setting_records if r["both_passed"])
    print(f"  {setting}: {passed}/{len(setting_records)} = {passed/len(setting_records)*100:.1f}%")

print()
print("=== ALL CHECKS PASSED ===")
```

Run this validation inline (not as a separate file). Print results. If any assertion fails, investigate and fix the collection script, then re-run.
  </action>
  <verify>
  - Validation script runs without assertion errors
  - All 5 requirement checks (DATA-01 through FIG4-02) print PASS
  - Schema completeness check shows all required fields present
  - Pass rates are printed for all 3 settings
  </verify>
  <done>data/results.json validated against all Phase 2 requirements. Unified store contains complete data for all settings with difficulty scores, bucket assignments, merge outcomes tracked separately from test outcomes, and infrastructure errors flagged.</done>
</task>

</tasks>

<verification>
1. `scripts/collect_results.py` exists and runs cleanly: `python scripts/collect_results.py`
2. `data/results.json` exists with expected record count
3. All records have non-null difficulty and bucket fields
4. All 3 settings present (solo, coop-comm, coop-nocomm)
5. LimitsExceeded pairs have both_passed=False
6. Merge outcomes correctly classified (solo=merge_clean, coop has merge_clean/merge_union/merge_failed)
7. Difficulty values are from {0.0, 0.33, 0.67, 1.0} (approximately, with rounding)
8. Bucket population reported for all 10 buckets (4 expected to be populated)
9. Coop-comm records have conversation messages embedded
</verification>

<success_criteria>
- data/results.json exists as a single flat JSON array
- Every record has all required schema fields (24 fields)
- 3 settings present: solo, coop-comm, coop-nocomm
- Difficulty scores from 3 solo seeds applied to ALL records
- 10 equal-width buckets assigned, population counts reported
- Infra errors flagged, LimitsExceeded overridden to both_passed=false
- Merge outcomes tracked independently from test outcomes
- scripts/collect_results.py is rerunnable (idempotent)
</success_criteria>

<output>
After completion, create `.planning/phases/02-results-collection-and-data-foundation/02-03-SUMMARY.md`
</output>
