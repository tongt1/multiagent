---
phase: 02-results-collection-and-data-foundation
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified: []
autonomous: true
requirements: [FIG4-01]

must_haves:
  truths:
    - "Two additional solo seed runs exist with 100 pairs each, matching the original 100 pairs"
    - "All 200 additional solo runs have eval.json files with test results"
    - "Three solo seeds total provide per-pair pass rate variance for difficulty scoring"
  artifacts:
    - path: "repos/CooperBench/logs/command-a-solo-seed1/"
      provides: "Solo seed 1 run results (100 pairs)"
    - path: "repos/CooperBench/logs/command-a-solo-seed2/"
      provides: "Solo seed 2 run results (100 pairs)"
    - path: "repos/CooperBench/logs/command-a-solo-seed1/eval_summary.json"
      provides: "Solo seed 1 eval summary"
    - path: "repos/CooperBench/logs/command-a-solo-seed2/eval_summary.json"
      provides: "Solo seed 2 eval summary"
  key_links:
    - from: "command-a-solo-seed1/solo/*/result.json"
      to: "command-a-solo/solo/*/result.json"
      via: "Same 100 feature pairs (deterministic from lite subset)"
      pattern: "result\\.json"
    - from: "command-a-solo-seed1/*/eval.json"
      to: "difficulty computation in Plan 03"
      via: "both_passed field used for d(pair) = 1 - mean(both_passed)"
      pattern: "both_passed"
---

<objective>
Run 2 additional solo seeds and evaluate them, providing 3 total solo data points per pair for meaningful difficulty score computation.

Purpose: With only 1 solo seed, difficulty is binary {0, 1}. With 3 seeds, difficulty takes values {0, 0.33, 0.67, 1.0}, populating 4 of 10 difficulty buckets and enabling the Figure 4 difficulty-stratified success curves to show meaningful trends. Per user decision, this costs ~$92 additional ($46/seed).

Output: 200 additional result.json + eval.json files across 2 new run directories.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-results-collection-and-data-foundation/02-RESEARCH.md
@.planning/phases/02-results-collection-and-data-foundation/02-01-SUMMARY.md
@scripts/run_cooperbench.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run 2 additional solo seeds on lite subset</name>
  <files></files>
  <action>
All commands must run from `repos/CooperBench/` directory. Environment variables from Phase 1 must be set (COHERE_API_KEY, MSWEA_MODEL_API_BASE, MSWEA_COST_TRACKING, LITELLM_LOG).

1. Verify environment is ready:
   ```bash
   cd /mnt/data/terry/home/cooperbench-repro/repos/CooperBench
   source .venv/bin/activate
   echo "COHERE_API_KEY: ${COHERE_API_KEY:+set}"
   echo "MSWEA_MODEL_API_BASE: ${MSWEA_MODEL_API_BASE:-NOT SET}"
   ```
   If MSWEA_MODEL_API_BASE is not set, export it:
   ```bash
   export MSWEA_MODEL_API_BASE="https://stg.api.cohere.com/v2/chat"
   export MSWEA_COST_TRACKING="ignore_errors"
   export LITELLM_LOG="ERROR"
   ```

2. First verify pair determinism -- do a dry check that the lite subset produces consistent pairs:
   ```bash
   python3 -c "
   from cooperbench.runner.tasks import load_subset
   data = load_subset('lite')
   total_pairs = sum(len(t.get('pairs', [])) for t in data.get('tasks', []))
   print(f'Lite subset: {len(data[\"tasks\"])} tasks, {total_pairs} pairs')
   "
   ```
   Expect: ~26 tasks, 100 pairs. If pair count differs from 100, STOP and investigate.

3. Run solo seed 1 (concurrency 4 to match Phase 1, --no-auto-eval since entrypoint fix already applied and we'll eval separately):
   ```bash
   cooperbench run -n command-a-solo-seed1 \
     -m command-a-03-2025 \
     -a mini_swe_agent \
     -s lite \
     --backend docker \
     -c 4 \
     --setting solo \
     --no-auto-eval
   ```

4. Run solo seed 2:
   ```bash
   cooperbench run -n command-a-solo-seed2 \
     -m command-a-03-2025 \
     -a mini_swe_agent \
     -s lite \
     --backend docker \
     -c 4 \
     --setting solo \
     --no-auto-eval
   ```

   NOTE: Seeds 1 and 2 can run sequentially (each takes ~45 minutes based on Phase 1 solo timing). They cannot run in parallel because they share the same Docker backend and 4 concurrent containers is already the tested limit.

5. Verify both seeds produced 100 result.json files:
   ```bash
   find logs/command-a-solo-seed1 -name "result.json" | wc -l   # expect 100
   find logs/command-a-solo-seed2 -name "result.json" | wc -l   # expect 100
   ```

6. Verify pair overlap with original solo run -- confirm the same 100 pairs were used:
   ```bash
   python3 -c "
   import json
   from pathlib import Path

   def get_pairs(run_name):
       pairs = set()
       for f in Path(f'logs/{run_name}').rglob('result.json'):
           d = json.load(open(f))
           pairs.add((d['repo'], d['task_id'], tuple(d['features'])))
       return pairs

   orig = get_pairs('command-a-solo')
   seed1 = get_pairs('command-a-solo-seed1')
   seed2 = get_pairs('command-a-solo-seed2')

   print(f'Original: {len(orig)} pairs')
   print(f'Seed 1:   {len(seed1)} pairs')
   print(f'Seed 2:   {len(seed2)} pairs')
   print(f'Overlap orig-seed1: {len(orig & seed1)}')
   print(f'Overlap orig-seed2: {len(orig & seed2)}')
   print(f'All 3 match: {orig == seed1 == seed2}')
   "
   ```
   If overlap is not 100%, note which pairs differ -- the collection script (Plan 03) will need to handle partial data.

NOTE: This task is LONG-RUNNING. Expect ~90 minutes total (45 min per seed). Cost: ~$92 ($46/seed based on Phase 1 solo cost).
  </action>
  <verify>
  - `find repos/CooperBench/logs/command-a-solo-seed1 -name "result.json" | wc -l` returns 100
  - `find repos/CooperBench/logs/command-a-solo-seed2 -name "result.json" | wc -l` returns 100
  - Pair overlap verification shows matching pairs across all 3 seeds
  </verify>
  <done>2 additional solo seed runs complete. 200 result.json files created across command-a-solo-seed1 and command-a-solo-seed2 directories.</done>
</task>

<task type="auto">
  <name>Task 2: Evaluate both additional solo seeds</name>
  <files></files>
  <action>
1. Clean up any leftover Docker containers:
   ```bash
   cd /mnt/data/terry/home/cooperbench-repro/repos/CooperBench
   docker ps -a --filter "ancestor=akhatua/cooperbench" -q | xargs -r docker rm -f
   ```

2. Run eval for both seeds in parallel (each at concurrency 10, for 20 total concurrent containers):
   ```bash
   source .venv/bin/activate
   cooperbench eval -n command-a-solo-seed1 --backend docker -c 10 &
   cooperbench eval -n command-a-solo-seed2 --backend docker -c 10 &
   wait
   ```

   If either fails, retry once:
   ```bash
   cooperbench eval -n command-a-solo-seed1 --backend docker -c 10
   cooperbench eval -n command-a-solo-seed2 --backend docker -c 10
   ```

3. Verify eval counts:
   ```bash
   find logs/command-a-solo-seed1 -name "eval.json" | wc -l   # expect 100
   find logs/command-a-solo-seed2 -name "eval.json" | wc -l   # expect 100
   ```

4. Report eval results:
   ```bash
   python3 -c "
   import json
   for name in ['command-a-solo-seed1', 'command-a-solo-seed2']:
       with open(f'logs/{name}/eval_summary.json') as f:
           s = json.load(f)
       print(f'{name}: {s[\"passed\"]} pass, {s[\"failed\"]} fail, {s[\"errors\"]} errors')
   "
   ```

5. Quick per-pair pass rate preview across all 3 seeds:
   ```bash
   python3 -c "
   import json
   from pathlib import Path
   from collections import defaultdict

   pair_passes = defaultdict(list)
   for run in ['command-a-solo', 'command-a-solo-seed1', 'command-a-solo-seed2']:
       for f in Path(f'logs/{run}').rglob('eval.json'):
           d = json.load(open(f))
           key = (d['repo'], d['task_id'], tuple(d['features']))
           pair_passes[key].append(d.get('both_passed', False))

   # Count difficulty values
   from collections import Counter
   diffs = Counter()
   for key, passes in pair_passes.items():
       d = round(1.0 - sum(passes)/len(passes), 2)
       diffs[d] += 1

   print(f'Pairs with 3 seeds: {sum(1 for v in pair_passes.values() if len(v) == 3)}')
   print(f'Difficulty distribution: {dict(sorted(diffs.items()))}')
   print(f'Expected values: 0.0 (easy), 0.33, 0.67, 1.0 (hard)')
   "
   ```

6. Clean up Docker containers:
   ```bash
   docker ps -a --filter "ancestor=akhatua/cooperbench" -q | xargs -r docker rm -f
   ```

NOTE: Eval takes ~30-60 minutes for 200 runs at concurrency 20.
  </action>
  <verify>
  - `find repos/CooperBench/logs/command-a-solo-seed1 -name "eval.json" | wc -l` returns 100
  - `find repos/CooperBench/logs/command-a-solo-seed2 -name "eval.json" | wc -l` returns 100
  - eval_summary.json exists for both seeds
  - Difficulty distribution preview shows values in {0.0, 0.33, 0.67, 1.0}
  </verify>
  <done>Both additional solo seeds evaluated. 200 eval.json files created. Combined with original solo, 3 seeds x 100 pairs = 300 solo eval points available for difficulty computation.</done>
</task>

</tasks>

<verification>
1. Total solo eval.json count:
   ```bash
   find repos/CooperBench/logs/command-a-solo* -name "eval.json" | wc -l
   ```
   Expect: 300 (100 per seed)

2. All 3 seeds cover the same 100 pairs (verified by pair overlap check in Task 1)

3. Difficulty distribution preview shows 4 distinct values {0.0, 0.33, 0.67, 1.0}
</verification>

<success_criteria>
- 200 additional result.json files (100 per seed) in command-a-solo-seed1/ and command-a-solo-seed2/
- 200 additional eval.json files (100 per seed)
- eval_summary.json for both new seeds
- All 3 solo seeds cover the same 100 feature pairs
- Combined with Plan 01, total of 500 eval.json files across all runs
</success_criteria>

<output>
After completion, create `.planning/phases/02-results-collection-and-data-foundation/02-02-SUMMARY.md`
</output>
